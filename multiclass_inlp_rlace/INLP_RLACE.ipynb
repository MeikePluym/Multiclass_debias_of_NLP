{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1e71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression, Lasso, Ridge, LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import copy\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import torch\n",
    "from torch.optim import SGD, Adam\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, neural_network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3478b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class Args():\n",
    "    \n",
    "    model_type='bert'\n",
    "    device=-1\n",
    "    run_id=1\n",
    "    do_inlp=1\n",
    "    do_rlace=1\n",
    "    ranks=\"[1,4,8,16,32,50,64,100]\"\n",
    "    pickle_file_train='gender_stereo_data_for_inlp_train'\n",
    "    pickle_file_dev='gender_stereo_data_for_inlp_dev'\n",
    "    pickle_file_test='gender_stereo_data_for_inlp_dev'\n",
    "    bias_type='gender_stereo'\n",
    "    \n",
    "args=Args()\n",
    "ranks=eval(args.ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2ad31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abstract class for linear classifiers\n",
    "class Classifier(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def train(self, X_train: np.ndarray, Y_train: np.ndarray, X_dev: np.ndarray,\n",
    "             Y_dev: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :return: accuracy score on the dev set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_weights(self) -> np.ndarray:\n",
    "        \"\"\" \n",
    "        :return: final weights of the model, as np array\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    \n",
    "class CovMaximizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def fit(self, X_train: np.ndarray, Y_train: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        :return: accuracy score on the dev set\n",
    "        \"\"\"\n",
    "        \n",
    "        eigvals, eigvecs = np.linalg.eigh((1./len(X_train)**2)*X_train.T@Y_train@Y_train.T@X_train)\n",
    "        \"\"\"\n",
    "        U, Sigma, VT = randomized_svd((1/np.sqrt(len(X_train)))*cov,\n",
    "                              n_components=2,\n",
    "                              n_iter=15,\n",
    "                              random_state=None)\n",
    "        \"\"\"\n",
    "        self.w = eigvecs[:,-1]\n",
    "        self.coef_ = self.w\n",
    "        print(self.w.shape)\n",
    "        \n",
    "    def score(self, X_dev, Y_dev):\n",
    "    \n",
    "        \n",
    "        cov = (1./len(X_dev)**2) * (X_dev.T@Y_dev@Y_dev.T@X_dev)\n",
    "        return self.w.T@cov.T@cov@self.w\n",
    "        \n",
    "    def get_weights(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :return: final weights of the model, as np array\n",
    "        \"\"\"\n",
    "\n",
    "        return self.w\n",
    "    \n",
    "class SKlearnClassifier(Classifier):\n",
    "\n",
    "    def __init__(self, m):\n",
    "\n",
    "        self.model = m\n",
    "\n",
    "    def train_network(self, X_train: np.ndarray, Y_train: np.ndarray, X_dev: np.ndarray, Y_dev: np.ndarray) -> float:\n",
    "\n",
    "        \"\"\"\n",
    "        :return: accuracy score on the dev set / Person's R in the case of regression\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.fit(X_train, Y_train)\n",
    "        score = self.model.score(X_dev, Y_dev)\n",
    "        return score\n",
    "\n",
    "    def get_weights(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :return: final weights of the model, as np array\n",
    "        \"\"\"\n",
    "\n",
    "        w = self.model.coef_\n",
    "        if len(w.shape) == 1:\n",
    "                w = np.expand_dims(w, 0)\n",
    "\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee2a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_debiasing_projection(classifier_class, cls_params: Dict, num_classifiers: int, input_dim: int,\n",
    "                            is_autoregressive: bool, min_accuracy: float, X_train: np.ndarray,\n",
    "                            Y_train: np.ndarray, X_dev: np.ndarray, Y_dev: np.ndarray, by_class=True,\n",
    "                            Y_train_main=None, Y_dev_main=None, dropout_rate=0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param classifier_class: the sklearn classifier class (SVM/Perceptron etc.)\n",
    "    :param cls_params: a dictionary, containing the params for the sklearn classifier\n",
    "    :param num_classifiers: number of iterations (equivalent to number of dimensions to remove)\n",
    "    :param input_dim: size of input vectors\n",
    "    :param is_autoregressive: whether to train the ith classiifer on the data projected to the nullsapces of w1,...,wi-1\n",
    "    :param min_accuracy: above this threshold, ignore the learned classifier\n",
    "    :param X_train: ndarray, training vectors\n",
    "    :param Y_train: ndarray, training labels (protected attributes)\n",
    "    :param X_dev: ndarray, eval vectors\n",
    "    :param Y_dev: ndarray, eval labels (protected attributes)\n",
    "    :param by_class: if true, at each iteration sample one main-task label, and extract the protected attribute only from vectors from this class\n",
    "    :param T_train_main: ndarray, main-task train labels\n",
    "    :param Y_dev_main: ndarray, main-task eval labels\n",
    "    :param dropout_rate: float, default: 0 (note: not recommended to be used with autoregressive=True)\n",
    "    :return: P, the debiasing projection; rowspace_projections, the list of all rowspace projection; Ws, the list of all calssifiers.\n",
    "    \"\"\"\n",
    "    if dropout_rate > 0 and is_autoregressive:\n",
    "        warnings.warn(\"Note: when using dropout with autoregressive training, the property w_i.dot(w_(i+1)) = 0 no longer holds.\")\n",
    "\n",
    "    I = np.eye(input_dim)\n",
    "\n",
    "    if by_class:\n",
    "        if ((Y_train_main is None) or (Y_dev_main is None)):\n",
    "            raise Exception(\"Need main-task labels for by-class training.\")\n",
    "        main_task_labels = list(set(Y_train_main.tolist()))\n",
    "\n",
    "    X_train_cp = X_train.copy()\n",
    "    X_dev_cp = X_dev.copy()\n",
    "    Y_train_cp = Y_train.copy()\n",
    "    Y_dev_cp = Y_dev.copy()\n",
    "    \n",
    "    rowspace_projections = []\n",
    "    Ws = []\n",
    "    accs = []\n",
    "    \n",
    "    pbar = tqdm(range(num_classifiers))\n",
    "    for i in pbar:\n",
    "\n",
    "        clf = SKlearnClassifier(classifier_class(**cls_params))\n",
    "        dropout_scale = 1./(1 - dropout_rate + 1e-6)\n",
    "        dropout_mask = (np.random.rand(*X_train.shape) < (1-dropout_rate)).astype(float) * dropout_scale\n",
    "\n",
    "\n",
    "        if by_class:\n",
    "            #cls = np.random.choice(Y_train_main)  # uncomment for frequency-based sampling\n",
    "            cls = random.choice(main_task_labels)\n",
    "            relevant_idx_train = Y_train_main == cls\n",
    "            relevant_idx_dev = Y_dev_main == cls\n",
    "        else:\n",
    "            relevant_idx_train = np.ones(X_train_cp.shape[0], dtype=bool)\n",
    "            relevant_idx_dev = np.ones(X_dev_cp.shape[0], dtype=bool)\n",
    "\n",
    "        acc = clf.train_network((X_train_cp * dropout_mask)[relevant_idx_train], Y_train_cp[relevant_idx_train], X_dev_cp[relevant_idx_dev], Y_dev_cp[relevant_idx_dev])\n",
    "        pbar.set_description(\"iteration: {}, accuracy: {}\".format(i, acc))\n",
    "        accs.append(acc)\n",
    "        if acc < min_accuracy: continue\n",
    "\n",
    "        W = clf.get_weights()\n",
    "        #W[0][-10:] = 0.0\n",
    "        \n",
    "        Ws.append(W)\n",
    "        P_rowspace_wi = get_rowspace_projection(W) # projection to W's rowspace\n",
    "        rowspace_projections.append(P_rowspace_wi)\n",
    "\n",
    "        if is_autoregressive:\n",
    "\n",
    "            \"\"\"\n",
    "            to ensure numerical stability, explicitly project to the intersection of the nullspaces found so far (instaed of doing X = P_iX,\n",
    "            which is problematic when w_i is not exactly orthogonal to w_i-1,...,w1, due to e.g inexact argmin calculation).\n",
    "            \"\"\"\n",
    "            # use the intersection-projection formula of Ben-Israel 2013 http://benisrael.net/BEN-ISRAEL-NOV-30-13.pdf:\n",
    "            # N(w1)∩ N(w2) ∩ ... ∩ N(wn) = N(P_R(w1) + P_R(w2) + ... + P_R(wn))\n",
    "\n",
    "            P = get_projection_to_intersection_of_nullspaces(rowspace_projections, input_dim)\n",
    "            # project\n",
    "\n",
    "\n",
    "\n",
    "            #Y_train_cp = Y_train_cp - (X_train_cp@W.T).squeeze()\n",
    "            #Y_dev_cp = Y_dev_cp - (X_dev_cp@W.T).squeeze()\n",
    "            \n",
    "            X_train_cp = (P.dot(X_train.T)).T\n",
    "            X_dev_cp = (P.dot(X_dev.T)).T\n",
    "         \n",
    "            \n",
    "\n",
    "    \"\"\"\n",
    "    calculate final projection matrix P=PnPn-1....P2P1\n",
    "    since w_i.dot(w_i-1) = 0, P2P1 = I - P1 - P2 (proof in the paper); this is more stable.\n",
    "    by induction, PnPn-1....P2P1 = I - (P1+..+PN). We will use instead Ben-Israel's formula to increase stability and also generalize to the non-orthogonal case (e.g. with dropout),\n",
    "    i.e., we explicitly project to intersection of all nullspaces (this is not critical at this point; I-(P1+...+PN) is roughly as accurate as this provided no dropout & regularization)\n",
    "    \"\"\"\n",
    "\n",
    "    P = get_projection_to_intersection_of_nullspaces(rowspace_projections, input_dim)\n",
    "    Ws = np.array(Ws).squeeze(1)\n",
    "    Ws = Ws/np.linalg.norm(Ws, axis = 1, keepdims=True)\n",
    "    \n",
    "    return P, accs, Ws\n",
    "\n",
    "\n",
    "\n",
    "def get_rowspace_projection(W:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param W: the matrix over its nullspace to project\n",
    "    :return: the projection matrix over the rowspace\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if np.allclose(W, 0):\n",
    "        w_basis = np.zeros_like(W.T)\n",
    "    else:\n",
    "        w_basis = scipy.linalg.orth(W.T) # orthogonal basis\n",
    "\n",
    "    P_W = w_basis.dot(w_basis.T) # orthogonal projection on W's rowspace\n",
    "\n",
    "    return P_W\n",
    "\n",
    "\n",
    "def get_projection_to_intersection_of_nullspaces(rowspace_projection_matrices: List[np.ndarray], input_dim: int):\n",
    "    \"\"\"\n",
    "    Given a list of rowspace projection matrices P_R(w_1), ..., P_R(w_n),\n",
    "    this function calculates the projection to the intersection of all nullspasces of the matrices w_1, ..., w_n.\n",
    "    uses the intersection-projection formula of Ben-Israel 2013 http://benisrael.net/BEN-ISRAEL-NOV-30-13.pdf:\n",
    "    N(w1)∩ N(w2) ∩ ... ∩ N(wn) = N(P_R(w1) + P_R(w2) + ... + P_R(wn))\n",
    "    :param rowspace_projection_matrices: List[np.array], a list of rowspace projections\n",
    "    :param dim: input dim\n",
    "    \"\"\"\n",
    "\n",
    "    I = np.eye(input_dim)\n",
    "    Q = np.sum(rowspace_projection_matrices, axis = 0)\n",
    "    P = I - get_rowspace_projection(Q)\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "def solve_adv_game(X_train, y_train, X_dev, y_dev, rank=1, device=\"cpu\", out_iters=75000,\n",
    "                  in_iters_adv=1, in_iters_clf=1,  epsilon=0.0015, batch_size=128, \n",
    "                   evalaute_every=1000, optimizer_class=SGD, \n",
    "                   optimizer_params_P={\"lr\": 0.005, \"weight_decay\": 1e-4}, \n",
    "                   optimizer_params_predictor={\"lr\": 0.005, \"weight_decay\": 1e-4}):\n",
    "    \"\"\"\n",
    "    :param X: The input (np array)\n",
    "    :param Y: the lables (np array)\n",
    "    :param X_dev: Dev set (np array)\n",
    "    :param Y_dev: Dev labels (np array)\n",
    "    :param rank: Number of dimensions to neutralize from the input.\n",
    "    :param device:\n",
    "    :param out_iters: Number of batches to run\n",
    "    :param in_iters_adv: number of iterations for adversary's optimization\n",
    "    :param in_iters_clf: number of iterations from the predictor's optimization\n",
    "    :param epsilon: stopping criterion .Stops if abs(acc - majority) < epsilon.\n",
    "    :param batch_size:\n",
    "    :param evalaute_every: After how many batches to evaluate the current adversary.\n",
    "    :param optimizer_class: SGD/Adam etc.\n",
    "    :param optimizer_params: the optimizer's params (as a dict)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def get_loss_fn(X, y, predictor, P, bce_loss_fn, optimize_P=False):\n",
    "        I = torch.eye(X_train.shape[1]).to(device)\n",
    "        bce = bce_loss_fn(predictor(X @ (I - P)).squeeze(), y)\n",
    "        if optimize_P:\n",
    "            bce = -bce\n",
    "        return bce\n",
    "\n",
    "\n",
    "    X_torch = torch.tensor(X_train).float().to(device)\n",
    "    y_torch = torch.tensor(y_train).float().to(device)\n",
    "\n",
    "    num_labels = len(set(y_train.tolist()))\n",
    "    if num_labels == 2:\n",
    "        predictor = torch.nn.Linear(X_train.shape[1], 1).to(device)\n",
    "        bce_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        y_torch = y_torch.float()\n",
    "    else:\n",
    "        predictor = torch.nn.Linear(X_train.shape[1], num_labels).to(device)\n",
    "        bce_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        y_torch = y_torch.long()\n",
    "\n",
    "    P = 1e-1*torch.randn(X_train.shape[1], X_train.shape[1]).to(device)\n",
    "    P.requires_grad = True\n",
    "\n",
    "    optimizer_predictor = optimizer_class(predictor.parameters(), **optimizer_params_predictor)\n",
    "    optimizer_P = optimizer_class([P],**optimizer_params_P)\n",
    "\n",
    "    maj = get_majority_acc(y_train)\n",
    "    label_entropy = get_entropy(y_train)\n",
    "    pbar = tqdm(range(out_iters), total = out_iters, ascii=True)\n",
    "    count_examples = 0\n",
    "    best_P, best_score, best_loss = None, 1, -1\n",
    "\n",
    "    for i in pbar:\n",
    "\n",
    "        for j in range(in_iters_adv):\n",
    "            P = symmetric(P)\n",
    "            optimizer_P.zero_grad()\n",
    "\n",
    "            idx = np.arange(0, X_torch.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_batch, y_batch = X_torch[idx[:batch_size]], y_torch[idx[:batch_size]]\n",
    "\n",
    "            loss_P = get_loss_fn(X_batch, y_batch, predictor, symmetric(P), bce_loss_fn, optimize_P=True)\n",
    "            loss_P.backward()\n",
    "            optimizer_P.step()\n",
    "\n",
    "            # project\n",
    "\n",
    "            with torch.no_grad():\n",
    "                D, U = torch.linalg.eigh(symmetric(P).detach().cpu())\n",
    "                D = D.detach().cpu().numpy()\n",
    "                D_plus_diag = solve_constraint(D, d=rank)\n",
    "                D = torch.tensor(np.diag(D_plus_diag).real).float().to(device)\n",
    "                U = U.to(device)\n",
    "                P.data = U @ D @ U.T\n",
    "\n",
    "        for j in range(in_iters_clf):\n",
    "            optimizer_predictor.zero_grad()\n",
    "            idx = np.arange(0, X_torch.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_batch, y_batch = X_torch[idx[:batch_size]], y_torch[idx[:batch_size]]\n",
    "\n",
    "            loss_predictor = get_loss_fn(X_batch, y_batch, predictor, symmetric(P), bce_loss_fn, optimize_P=False)\n",
    "            loss_predictor.backward()\n",
    "            optimizer_predictor.step()\n",
    "            count_examples += batch_size\n",
    "\n",
    "        if i % evalaute_every == 0:\n",
    "            #pbar.set_description(\"Evaluating current adversary...\")\n",
    "            loss_val, score = get_score(X_train, y_train, X_train, y_train, P.detach().cpu().numpy(), rank)\n",
    "            if loss_val > best_loss:#if np.abs(score - maj) < np.abs(best_score - maj):\n",
    "                best_P, best_loss = symmetric(P).detach().cpu().numpy().copy(), loss_val\n",
    "            if np.abs(score - maj) < np.abs(best_score - maj):\n",
    "                best_score = score\n",
    "                \n",
    "            # update progress bar\n",
    "            \n",
    "            best_so_far = best_score if np.abs(best_score-maj) < np.abs(score-maj) else score\n",
    "            \n",
    "            pbar.set_description(\"{:.0f}/{:.0f}. Acc post-projection: {:.3f}%; best so-far: {:.3f}%; Maj: {:.3f}%; Gap: {:.3f}%; best loss: {:.4f}; current loss: {:.4f}\".format(i, out_iters, score * 100, best_so_far * 100, maj * 100, np.abs(best_so_far - maj) * 100, best_loss, loss_val))\n",
    "            pbar.refresh()  # to show immediately the update\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        if i > 1 and np.abs(best_score - maj) < epsilon:\n",
    "        #if i > 1 and np.abs(best_loss - label_entropy) < epsilon:\n",
    "                    break\n",
    "    output = prepare_output(best_P,rank,best_score)\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_majority_acc(y):\n",
    "    c = Counter(y)\n",
    "    fracts = [v / sum(c.values()) for v in c.values()]\n",
    "    maj = max(fracts)\n",
    "    return maj\n",
    "\n",
    "\n",
    "def get_entropy(y):\n",
    "    c = Counter(y)\n",
    "    fracts = [v / sum(c.values()) for v in c.values()]\n",
    "    return scipy.stats.entropy(fracts)\n",
    "\n",
    "def prepare_output(P,rank,score):\n",
    "    P_final = get_projection(P,rank)\n",
    "    return {\"score\": score, \"P_before_svd\": np.eye(P.shape[0]) - P, \"P\": P_final}\n",
    "\n",
    "\n",
    "def solve_constraint(lambdas, d=1):\n",
    "    def f(theta):\n",
    "        return_val = np.sum(np.minimum(np.maximum(lambdas - theta, 0), 1)) - d\n",
    "        return return_val\n",
    "\n",
    "    theta_min, theta_max = max(lambdas), min(lambdas) - 1\n",
    "    assert f(theta_min) * f(theta_max) < 0\n",
    "\n",
    "    mid = (theta_min + theta_max) / 2\n",
    "    tol = 1e-4\n",
    "    iters = 0\n",
    "\n",
    "    while iters < 25:\n",
    "\n",
    "        mid = (theta_min + theta_max) / 2\n",
    "\n",
    "        if f(mid) * f(theta_min) > 0:\n",
    "\n",
    "            theta_min = mid\n",
    "        else:\n",
    "            theta_max = mid\n",
    "        iters += 1\n",
    "\n",
    "    lambdas_plus = np.minimum(np.maximum(lambdas - mid, 0), 1)\n",
    "    # if (theta_min-theta_max)**2 > tol:\n",
    "    #    print(\"didn't converge\", (theta_min-theta_max)**2)\n",
    "    return lambdas_plus\n",
    "\n",
    "def get_score(X_train, y_train, X_dev, y_dev, P, rank):\n",
    "    P_svd = get_projection(P, rank)\n",
    "    \n",
    "    loss_vals = []\n",
    "    accs = []\n",
    "    \n",
    "    for i in range(NUM_CLFS_IN_EVAL):\n",
    "        clf = init_classifier()\n",
    "        clf.fit(X_train@P_svd, y_train)\n",
    "        y_pred = clf.predict_proba(X_dev@P_svd)\n",
    "        loss = sklearn.metrics.log_loss(y_dev, y_pred)\n",
    "        loss_vals.append(loss)\n",
    "        accs.append(clf.score(X_dev@P_svd, y_dev))\n",
    "        \n",
    "    i = np.argmin(loss_vals)\n",
    "    return loss_vals[i], accs[i]\n",
    "\n",
    "\n",
    "def init_classifier():\n",
    "\n",
    "    return SGDClassifier(loss=EVAL_CLF_PARAMS[\"loss\"], fit_intercept=True, max_iter=EVAL_CLF_PARAMS[\"max_iter\"], tol=EVAL_CLF_PARAMS[\"tol\"], n_iter_no_change=EVAL_CLF_PARAMS[\"iters_no_change\"],\n",
    "                        n_jobs=32, alpha=EVAL_CLF_PARAMS[\"alpha\"])\n",
    "                        \n",
    "def symmetric(X):\n",
    "    X.data = 0.5 * (X.data + X.data.T)\n",
    "    return X\n",
    "\n",
    "def get_projection(P, rank):\n",
    "    D,U = np.linalg.eigh(P)\n",
    "    U = U.T\n",
    "    W = U[-rank:]\n",
    "    P_final = np.eye(P.shape[0]) - W.T @ W\n",
    "    return P_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fec43a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "def load_data(args, file, seed=None):\n",
    "    \n",
    "    X=np.load(\"encodings/{}/{}.npy\".format(args.model_type, file))\n",
    "    \n",
    "    #EXPAND WHEN ADDING ADDITIONAL BIAS DIRECTIONS\n",
    "    with open(\"./data_for_inlp/{}/{}.pickle\".format(args.model_type, file), \"rb\") as f:\n",
    "        data=pickle.load(f)\n",
    "        Y=np.array([1 if d[\"g\"]==\"m\" else 0 for d in data])\n",
    "        stereotype=np.array([d[\"s\"] for d in data])\n",
    "        txts=[d[\"text\"] for d in data]\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        X, Y, stereotype, txts, data = sklearn.utils.shuffle(X,Y,stereotype,txts,data)\n",
    "        X=X[:]\n",
    "        Y=Y[:]\n",
    "        \n",
    "    return X,Y,txts,stereotype,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319b0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inlp(X, y, X_dev, y_dev, num_iters=25):\n",
    "    clf=SGDClassifier\n",
    "    LOSS=\"log\"\n",
    "    ALPHA = 1e-4\n",
    "    TOL = 1e-4\n",
    "    ITER_NO_CHANGE = 20\n",
    "    params = {\"loss\": LOSS, \"fit_intercept\": True, \"max_iter\": 2000000, \"tol\": TOL, \"n_iter_no_change\": ITER_NO_CHANGE,\n",
    "              \"alpha\": ALPHA, \"n_jobs\": 32}\n",
    "    \n",
    "    input_dim=X_dev.shape[1]\n",
    "    \n",
    "    P_inlp, accs_inlp, ws_inlp_normalized = get_debiasing_projection(clf, params, num_iters, X_dev.shape[1], True, -1,\n",
    "                                                                     X, y, X_dev, y_dev, by_class=False,\n",
    "                                                                     Y_train_main=None, Y_dev_main=None, dropout_rate=0)\n",
    "    \n",
    "    Ps_nullspace=[]\n",
    "    for i in range(1, num_iters):\n",
    "        P=np.eye(X.shape[1]) - (ws_inlp_normalized[:i]).T @ ws_inlp_normalized[:i]\n",
    "        Ps_nullspace.append(P)\n",
    "        \n",
    "    for P, w in zip(Ps_nullspace, ws_inlp_normalized):\n",
    "        assert np.allclose(X @ P @ w, np.zeros(X.shape[0]))\n",
    "\n",
    "    return Ps_nullspace, ws_inlp_normalized, accs_inlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f36b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(X, y, path, title, method=\"pca\"):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        plt.rcParams['font.family']='Serif'\n",
    "\n",
    "        if method == \"pca\":\n",
    "            pca = PCA(n_components=2)\n",
    "            M = 6000\n",
    "        elif method == \"tsne\":\n",
    "            pca = TSNE(n_components=2, learning_rate=\"auto\", init=\"pca\")\n",
    "            M = 1500\n",
    "\n",
    "        X_proj = pca.fit_transform(X[:M])\n",
    "        ax = plt.axes()\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "        ax.xaxis.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "        y_text = [\"Female-biased\" if yy == 1 else \"Male-biased\" for yy in y]\n",
    "        plt1 = sn.scatterplot(X_proj[:, 0], X_proj[:, 1], hue=y_text[:M])\n",
    "        plt.legend(fontsize=19)\n",
    "        ax.set_title('{}'.format(title), fontsize=25)\n",
    "        ax.figure.savefig(\"{}/{}.pdf\".format(path, title), dpi=400)\n",
    "        plt.clf()\n",
    "        \n",
    "def get_svd(X):\n",
    "    D, U = np.linalg.eigh(X)\n",
    "    return U, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8580762",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_CLF_PARAMS = {\"loss\": \"log\", \"tol\": 1e-4, \"iters_no_change\": 15, \"alpha\": 1e-4, \"max_iter\": 25000}\n",
    "NUM_CLFS_IN_EVAL = 3 # change to 1 for large dataset / high dimensionality\n",
    "\n",
    "rlace_projs=defaultdict(dict)\n",
    "inlp_projs=defaultdict(dict)\n",
    "device=\"cpu\" if args.device==-1 else \"cuda:{}\".format(args.device)\n",
    "DEVICE=device\n",
    "\n",
    "X, y, txts, stereotypes, data=load_data(args, args.pickle_file_train)\n",
    "if len(X)>=100000:\n",
    "    X, y=X[:100000], y[:100000]\n",
    "    \n",
    "if not os.path.exists(\"pca\"):\n",
    "    os.makedirs(\"pca\")\n",
    "pca=PCA(random_state=args.run_id, n_components=10)\n",
    "pca.fit(X)\n",
    "with open(\"pca/pca_{}_{}.pickle\".format(args.model_type, args.run_id), \"wb\") as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "\n",
    "X_dev, y_dev, txts_dev, stereotypes_dev, data_dev=load_data(args, args.pickle_file_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46868e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"plots/{}/original/pca\".format(args.model_type), exist_ok=True)\n",
    "os.makedirs(\"plots/{}/original/tsne\".format(args.model_type), exist_ok=True)\n",
    "os.makedirs(\"plots/{}/inlp/pca\".format(args.model_type), exist_ok=True)\n",
    "os.makedirs(\"plots/{}/inlp/tsne\".format(args.model_type), exist_ok=True)\n",
    "os.makedirs(\"plots/{}/rlace/pca\".format(args.model_type), exist_ok=True)\n",
    "os.makedirs(\"plots/{}/rlace/tsne\".format(args.model_type), exist_ok=True)\n",
    "os.makedirs(\"interim/{}/rlace\".format(args.model_type), exist_ok=True)\n",
    "os.makedirs(\"interim/{}/inlp\".format(args.model_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d039a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 100, accuracy: 0.8673575129533678: 100%|██████████| 101/101 [10:03<00:00,  5.97s/it]\n"
     ]
    }
   ],
   "source": [
    "if args.do_inlp==1:\n",
    "    Ps_nullspace_inlp, ws_inlp_normalized, accs_inlp=run_inlp(X, y, X_dev, y_dev, num_iters=101)\n",
    "\n",
    "    if not os.path.exists(\"interim/{}/inlp\".format(args.model_type)):\n",
    "        os.makedirs(\"interim/{}/inlp\".format(args.model_type))\n",
    "\n",
    "    with open(\"interim/{}/inlp/Ps_inlp.pickle\".format(args.model_type), \"wb\") as f:\n",
    "        pickle.dump((Ps_nullspace_inlp, accs_inlp), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "134320ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/60000. Acc post-projection: 83.253%; best so-far: 83.253%; Maj: 83.253%; Gap: 0.000%; best loss: 0.4472; current loss: 0.4472:   0%|          | 2/60000 [00:15<127:25:39,  7.65s/it]\n",
      "0/60000. Acc post-projection: 83.253%; best so-far: 83.253%; Maj: 83.253%; Gap: 0.000%; best loss: 0.4420; current loss: 0.4420:   0%|          | 2/60000 [00:17<148:46:41,  8.93s/it]\n",
      "0/60000. Acc post-projection: 83.253%; best so-far: 83.253%; Maj: 83.253%; Gap: 0.000%; best loss: 0.4404; current loss: 0.4404:   0%|          | 2/60000 [00:17<142:59:45,  8.58s/it]\n",
      "0/60000. Acc post-projection: 83.253%; best so-far: 83.253%; Maj: 83.253%; Gap: 0.000%; best loss: 0.4419; current loss: 0.4419:   0%|          | 2/60000 [00:18<150:32:53,  9.03s/it]\n",
      "0/60000. Acc post-projection: 83.253%; best so-far: 83.253%; Maj: 83.253%; Gap: 0.000%; best loss: 0.4378; current loss: 0.4378:   0%|          | 2/60000 [00:19<165:04:39,  9.90s/it]\n",
      "0/60000. Acc post-projection: 83.253%; best so-far: 83.253%; Maj: 83.253%; Gap: 0.000%; best loss: 0.4806; current loss: 0.4806:   0%|          | 2/60000 [00:14<121:29:40,  7.29s/it]\n",
      "0/60000. Acc post-projection: 83.241%; best so-far: 83.241%; Maj: 83.253%; Gap: 0.012%; best loss: 0.4398; current loss: 0.4398:   0%|          | 2/60000 [00:15<129:39:11,  7.78s/it]\n",
      "0/60000. Acc post-projection: 83.253%; best so-far: 83.253%; Maj: 83.253%; Gap: 0.000%; best loss: 0.4438; current loss: 0.4438:   0%|          | 2/60000 [00:16<137:50:42,  8.27s/it]\n"
     ]
    }
   ],
   "source": [
    "Ps_rlace, accs_rlace = {}, {}\n",
    "if not args.do_rlace==1:\n",
    "    exit()\n",
    "\n",
    "optimizer_class=torch.optim.SGD\n",
    "optimizer_params_P = {\"lr\": 0.005, \"weight_decay\": 1e-4, \"momentum\": 0.0}\n",
    "optimizer_params_predictor = {\"lr\": 0.005, \"weight_decay\": 1e-5, \"momentum\": 0.9}\n",
    "\n",
    "for rank in ranks:\n",
    "\n",
    "    output = solve_adv_game(X, y, X, y, rank=rank, device=DEVICE, out_iters=60000,\n",
    "                                optimizer_class=optimizer_class, optimizer_params_P=optimizer_params_P,\n",
    "                                optimizer_params_predictor=optimizer_params_predictor, epsilon=0.002,\n",
    "                                batch_size=256)\n",
    "\n",
    "    P = output[\"P\"]\n",
    "    Ps_rlace[rank] = P\n",
    "    accs_rlace[rank] = output[\"score\"]\n",
    "\n",
    "    if not os.path.exists(\"interim/{}/rlace\".format(args.model_type)):\n",
    "        os.makedirs(\"interim/{}/rlace\".format(args.model_type))\n",
    "\n",
    "    with open(\"interim/{}/rlace/Ps_rlace.pickle\".format(args.model_type), \"wb\") as f:\n",
    "        pickle.dump((Ps_rlace, accs_rlace), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save projection matrices to file:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def load_data(args, file):\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        data=pickle.load(f)\n",
    "        z=np.array([1 if d[\"g\"]==\"m\" else 0 for d in data])\n",
    "        stereotypes=np.array([d[\"s\"] for d in data])\n",
    "        txts=[d[\"text\"] for d in data]\n",
    "\n",
    "    return z, txts, stereotypes, data\n",
    "\n",
    "def load_data_representations(args, file, seed=0):\n",
    "\n",
    "    X=np.load(\"encodings/{}/{}.npy\".format(args.model_type, file))\n",
    "\n",
    "    with open(\"pca/pca_{}_{}.pickle\".format(args.model_type, args.run_id), \"rb\") as f:\n",
    "        pca=pickle.load(f)\n",
    "    #X=pca.transform(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def load_projections(proj_type, seed=0):\n",
    "\n",
    "    with open(\"interim/{}/{}/Ps_{}.pickle\".format(args.model_type, proj_type, proj_type), \"rb\") as f:\n",
    "        rank2P=pickle.load(f)\n",
    "\n",
    "        return rank2P"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "pickle_file_train=\"./data_for_inlp/{}/{}.pickle\".format(args.model_type, args.pickle_file_train)\n",
    "pickle_file_dev=\"./data_for_inlp/{}/{}.pickle\".format(args.model_type, args.pickle_file_dev)\n",
    "pickle_file_test=\"./data_for_inlp/{}/{}.pickle\".format(args.model_type, args.pickle_file_test)\n",
    "\n",
    "z_train,txts_train,stereotypes_train,train = load_data(args, pickle_file_train)\n",
    "z_test,txts_test,stereotypes_test,test = load_data(args, pickle_file_test)\n",
    "z_dev,txts_dev,stereotypes_dev,dev = load_data(args, pickle_file_dev)\n",
    "\n",
    "if False and os.path.exists(\"analysis/{}\".format(args.model_type)) and os.path.exists(\"analysis/{}/mode2x.pickle\".format(args.model_type)) and os.path.exists(\"analysis/{}/mode2p.pickle\".format(args.model_type)):\n",
    "    with open(\"analysis/{}/mode2p.pickle\".format(args.model_type), \"rb\") as f:\n",
    "        mode2p = pickle.load(f)\n",
    "    with open(\"analysis/{}/mode2x.pickle\".format(args.model_type), \"rb\") as f:\n",
    "        mode2x = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(\"analysis/{}\".format(args.model_type)): os.mkdir(\"analysis/{}\".format(args.model_type))\n",
    "    mode2x = defaultdict(dict)\n",
    "    mode2p = defaultdict(dict)\n",
    "\n",
    "    mode2x[\"train\"]=load_data_representations(args, args.pickle_file_train)\n",
    "    mode2x[\"dev\"]=load_data_representations(args, args.pickle_file_dev)\n",
    "    mode2x[\"test\"]=load_data_representations(args, args.pickle_file_test)\n",
    "\n",
    "    for projtype in [\"rlace\", \"inlp\"]:\n",
    "        rank2P=load_projections(projtype)\n",
    "        mode2p[projtype]=rank2P\n",
    "\n",
    "    with open(\"analysis/{}/mode2x_{}.pickle\".format(args.model_type, args.bias_type), \"wb\") as f:\n",
    "        pickle.dump(mode2x, f)\n",
    "    with open(\"analysis/{}/mode2p_{}.pickle\".format(args.model_type, args.bias_type), \"wb\") as f:\n",
    "        pickle.dump(mode2p, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}