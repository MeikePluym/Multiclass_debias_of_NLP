{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e981d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import regex as re\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import *\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476b1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_model_type='bert' #from ['bert', 'roberta', 'albert', 'dbert', 'electra', 'gpt2']\n",
    "this_block_size=128\n",
    "attributes=['gender.txt']\n",
    "stereotypes_file='gender_stereotype.txt'  #not using stereotypes; define ''\n",
    "out_combo='gender_stereotype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72fd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('news_commentary_v15.en'):\n",
    "    f_in=gzip.open('news-commentary-v15.en.gz') #download from website; or use curl\n",
    "    f_out=open('news_commentary_v15.en', 'wb')\n",
    "    f_out.writelines(f_in)\n",
    "    f_out.close()\n",
    "    f_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b1bcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_commentary_v15.en', 'r', encoding='utf-8') as f:\n",
    "    lines=f.readlines()\n",
    "\n",
    "data=[l.strip() for l in lines]\n",
    "\n",
    "if stereotypes_file:\n",
    "    stereotypes=[word.strip() for word in open(stereotypes_file)]\n",
    "    stereotype_set=set(stereotypes)\n",
    "\n",
    "pat=re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "attributes_l=[]\n",
    "all_attributes_set=set()\n",
    "for attribute in attributes:\n",
    "    l=[word.strip() for word in open(attribute)]\n",
    "    attributes_l.append(set(l))\n",
    "    all_attributes_set |= set(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1355729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transformer(model_type):\n",
    "    if model_type=='bert':\n",
    "        pretrained_weights='bert-base-uncased'\n",
    "        model=BertModel.from_pretrained(pretrained_weights, output_hidden_states=True)\n",
    "        tokenizer=BertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='roberta':\n",
    "        pretrained_weights='roberta-base'\n",
    "        model=RobertaModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=RobertaTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='albert':\n",
    "        pretrained_weights='albert-base-v2'\n",
    "        model=AlbertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=AlbertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='dbert':\n",
    "        pretrained_weights='distilbert-base-uncased'\n",
    "        model=DistilBertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xlnet':\n",
    "        pretrained_weights='xlnet-base-cased'\n",
    "        model=XLNetModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=XLNetTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='electra':\n",
    "        pretrained_weights='google/electra-small-discriminator'\n",
    "        model=ElectraModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=ElectraTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt':\n",
    "        pretrained_weights='openai-gpt'\n",
    "        model=OpenAIGPTModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=OpenAIGPTTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt2':\n",
    "        pretrained_weights='gpt2'\n",
    "        model=GPT2Model.from_pretrained(pretrained_weights)\n",
    "        tokenizer=GPT2Tokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xl':\n",
    "        pretrained_weights='transfo-xl-wt103'\n",
    "        model=TransfoXLModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=TransfoXLTokenizer.from_pretrained(pretrained_weights)\n",
    "    return model, tokenizer\n",
    "\n",
    "def encode_to_is(tokenizer, the_data, add_special_tokens):\n",
    "    if type(the_data)==list:\n",
    "        data=[tuple(tokenizer.encode(sentence, add_special_tokens=add_special_tokens)) for sentence in the_data]\n",
    "    elif type(the_data)==dict:\n",
    "        data={tuple(tokenizer.encode(key, add_special_tokens=add_special_tokens)): tokenizer.encode(value, add_special_tokens=add_special_tokens) for key, value in the_data.items()}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763bcc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\466476mp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\466476mp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\466476mp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\466476mp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\466476mp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer=prepare_transformer(this_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2350b38c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608912/608912 [01:04<00:00, 9420.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral: 9974\n",
      "attributes0: 56800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if stereotypes_file:\n",
    "    tok_stereotypes=encode_to_is(tokenizer, stereotypes, add_special_tokens=False)\n",
    "\n",
    "neutral_examples=[]\n",
    "neutral_labels=[]\n",
    "stereotype_attr_labels=[]\n",
    "attribute_examples=[[] for _ in range(len(attributes_l))]\n",
    "attribute_labels=[[] for _ in range(len(attributes_l))]\n",
    "\n",
    "\n",
    "for line in tqdm(data):\n",
    "    neutral_flag=True\n",
    "    line=line.strip()\n",
    "    if len(line)<1:\n",
    "        continue\n",
    "    length=len(line.split())\n",
    "    if length>this_block_size or length<=1:\n",
    "        continue\n",
    "    tokens_orig=[token.strip() for token in re.findall(pat, line)]\n",
    "    tokens_lower=[token.lower() for token in tokens_orig]\n",
    "    token_set=set(tokens_lower)\n",
    "    \n",
    "    attribute_other_l=[]\n",
    "    for i, _ in enumerate(attributes_l):\n",
    "        a_set=set()\n",
    "        for j, attribute in enumerate(attributes_l):\n",
    "            if i!=j:\n",
    "                a_set |= attribute\n",
    "        attribute_other_l.append(a_set)\n",
    "    \n",
    "    for i, (attribute_set, other_set) in enumerate(zip(attributes_l, attribute_other_l)):\n",
    "        # & is bitwise AND operator\n",
    "        if attribute_set & token_set: #if a gender attribute is in the data line; classify the line as not neutral; and set the attribute to be the label\n",
    "            neutral_flag=False\n",
    "            if not other_set&token_set:\n",
    "                orig_line=line\n",
    "                line=tokenizer.encode(line, add_special_tokens=True)\n",
    "                labels=attribute_set & token_set\n",
    "                for label in list(labels):\n",
    "                    idx=tokens_lower.index(label)\n",
    "                label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "                line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                if label not in line_ngram:\n",
    "                    label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                    line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram:\n",
    "                        label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                        line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                        if label not in line_ngram:\n",
    "                            label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                            line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                idx=line_ngram.index(label)\n",
    "                attribute_examples[i].append(line)\n",
    "                attribute_labels[i].append([idx+j for j in range(len(label))])\n",
    "                attr_label=label\n",
    "                \n",
    "    if not neutral_flag and stereotype_set&token_set:\n",
    "        line=orig_line\n",
    "        line=tokenizer.encode(line, add_special_tokens=False)\n",
    "        neutr_labels=stereotype_set&token_set\n",
    "        for label in list(neutr_labels):\n",
    "            stereotype_attr_labels.append(attr_label)\n",
    "            idx=tokens_lower.index(label)\n",
    "            label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "            line_ngram_neutral=list(nltk.ngrams(line, len(label)))\n",
    "            if label not in line_ngram_neutral:\n",
    "                label = tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "                if label not in line_ngram_neutral:\n",
    "                    label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                    line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram_neutral:\n",
    "                        label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                        line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "            idx=line_ngram_neutral.index(label)\n",
    "            neutral_examples.append(line)\n",
    "            neutral_labels.append(label)\n",
    "            #neutral_labels.append([idx+i for i in range(len(label))])\n",
    "                    \n",
    "                \n",
    "print('neutral:', len(neutral_examples))\n",
    "for i, examples in enumerate(attribute_examples):\n",
    "    print(f'attributes{i}:', len(examples))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bdffdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neutral_examples)):\n",
    "    neutral_examples[i]=tokenizer.decode(neutral_examples[i])\n",
    "    neutral_labels[i]=tokenizer.decode(neutral_labels[i])\n",
    "    stereotype_attr_labels[i]=tokenizer.decode(stereotype_attr_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37968f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d87f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"gender_with_labels.pickle\", \"rb\")\n",
    "gender_dict=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aef022f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_data=[]\n",
    "for i in range(len(neutral_examples)):\n",
    "    this_line=defaultdict()\n",
    "    this_line['g']=gender_dict[stereotype_attr_labels[i]]\n",
    "    this_line['s']=neutral_labels[i]\n",
    "    this_line['text']=neutral_examples[i]\n",
    "    gender_data.append(this_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "428b582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_data_train=gender_data[:round(len(gender_data)*0.9)]\n",
    "gender_data_dev=gender_data[round(len(gender_data)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c43fc9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gender_data_for_inlp_train.pickle', 'wb') as f:\n",
    "    pickle.dump(gender_data_train, f)\n",
    "    \n",
    "with open('gender_data_for_inlp_dev.pickle', 'wb') as f:\n",
    "    pickle.dump(gender_data_dev, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b502a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
