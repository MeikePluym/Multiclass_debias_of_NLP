{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e981d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import regex as re\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import *\n",
    "import os.path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "476b1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_model_type='albert' #from ['bert', 'roberta', 'albert', 'dbert', 'electra', 'gpt2']\n",
    "this_block_size=128\n",
    "attributes=['./attribute_target_words/attributes/ethnicities_colour.txt']\n",
    "stereotypes_file='./attribute_target_words/targets/polarized_and_class_words.txt'  #not using stereotypes; define ''\n",
    "\n",
    "attr_w_labels='ethnicities_w_label.txt'\n",
    "\n",
    "output_file_train='ethnicities_pol_data_for_inlp_train'\n",
    "train_dev_split=0.9\n",
    "output_file_dev='ethnicities_pol_data_for_inlp_dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "text_file_w_labels='./attribute_target_words/with_label/{}'.format(attr_w_labels)\n",
    "data_w_labels=defaultdict()\n",
    "\n",
    "data=[word.strip().split() for word in open(text_file_w_labels)]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data_w_labels[data[i][1]]=data[i][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c72fd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('news_commentary_v15.en'):\n",
    "    f_in=gzip.open('news-commentary-v15.en.gz') #download from website; or use curl\n",
    "    f_out=open('news_commentary_v15.en', 'wb')\n",
    "    f_out.writelines(f_in)\n",
    "    f_out.close()\n",
    "    f_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b1bcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_commentary_v15.en', 'r', encoding='utf-8') as f:\n",
    "    lines=f.readlines()\n",
    "\n",
    "data=[l.strip() for l in lines]\n",
    "\n",
    "if stereotypes_file:\n",
    "    stereotypes=[word.strip() for word in open(stereotypes_file)]\n",
    "    stereotype_set=set(stereotypes)\n",
    "\n",
    "pat=re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "attributes_l=[]\n",
    "all_attributes_set=set()\n",
    "for attribute in attributes:\n",
    "    l=[word.strip() for word in open(attribute)]\n",
    "    attributes_l.append(set(l))\n",
    "    all_attributes_set |= set(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1355729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transformer(model_type):\n",
    "    if model_type=='bert':\n",
    "        pretrained_weights='bert-base-uncased'\n",
    "        model=BertModel.from_pretrained(pretrained_weights, output_hidden_states=True)\n",
    "        tokenizer=BertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='roberta':\n",
    "        pretrained_weights='roberta-base'\n",
    "        model=RobertaModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=RobertaTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='albert':\n",
    "        pretrained_weights='albert-base-v2'\n",
    "        model=AlbertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=AlbertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='dbert':\n",
    "        pretrained_weights='distilbert-base-uncased'\n",
    "        model=DistilBertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xlnet':\n",
    "        pretrained_weights='xlnet-base-cased'\n",
    "        model=XLNetModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=XLNetTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='electra':\n",
    "        pretrained_weights='google/electra-small-discriminator'\n",
    "        model=ElectraModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=ElectraTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt':\n",
    "        pretrained_weights='openai-gpt'\n",
    "        model=OpenAIGPTModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=OpenAIGPTTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt2':\n",
    "        pretrained_weights='gpt2'\n",
    "        model=GPT2Model.from_pretrained(pretrained_weights)\n",
    "        tokenizer=GPT2Tokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xl':\n",
    "        pretrained_weights='transfo-xl-wt103'\n",
    "        model=TransfoXLModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=TransfoXLTokenizer.from_pretrained(pretrained_weights)\n",
    "    return model, tokenizer\n",
    "\n",
    "def encode_to_is(tokenizer, the_data, add_special_tokens):\n",
    "    if type(the_data)==list:\n",
    "        data=[tuple(tokenizer.encode(sentence, add_special_tokens=add_special_tokens)) for sentence in the_data]\n",
    "    elif type(the_data)==dict:\n",
    "        data={tuple(tokenizer.encode(key, add_special_tokens=add_special_tokens)): tokenizer.encode(value, add_special_tokens=add_special_tokens) for key, value in the_data.items()}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "763bcc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of AlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n",
      "loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer=prepare_transformer(this_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2350b38c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608912/608912 [00:20<00:00, 29789.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral: 206\n",
      "attributes0: 3594\n"
     ]
    }
   ],
   "source": [
    "if stereotypes_file:\n",
    "    tok_stereotypes=encode_to_is(tokenizer, stereotypes, add_special_tokens=False)\n",
    "\n",
    "neutral_examples=[]\n",
    "neutral_labels=[]\n",
    "stereotype_attr_labels=[]\n",
    "attribute_examples=[[] for _ in range(len(attributes_l))]\n",
    "attribute_labels=[[] for _ in range(len(attributes_l))]\n",
    "\n",
    "\n",
    "for line in tqdm(data):\n",
    "    neutral_flag=True\n",
    "    line=line.strip()\n",
    "    if len(line)<1:\n",
    "        continue\n",
    "    length=len(line.split())\n",
    "    if length>this_block_size or length<=1:\n",
    "        continue\n",
    "    tokens_orig=[token.strip() for token in re.findall(pat, line)]\n",
    "    tokens_lower=[token.lower() for token in tokens_orig]\n",
    "    token_set=set(tokens_lower)\n",
    "    \n",
    "    attribute_other_l=[]\n",
    "    for i, _ in enumerate(attributes_l):\n",
    "        a_set=set()\n",
    "        for j, attribute in enumerate(attributes_l):\n",
    "            if i!=j:\n",
    "                a_set |= attribute\n",
    "        attribute_other_l.append(a_set)\n",
    "    \n",
    "    for i, (attribute_set, other_set) in enumerate(zip(attributes_l, attribute_other_l)):\n",
    "        # & is bitwise AND operator\n",
    "        if attribute_set & token_set: #if a gender attribute is in the data line; classify the line as not neutral; and set the attribute to be the label\n",
    "            neutral_flag=False\n",
    "            if not other_set&token_set:\n",
    "                orig_line=line\n",
    "                line=tokenizer.encode(line, add_special_tokens=True)\n",
    "                labels=attribute_set & token_set\n",
    "                for label in list(labels):\n",
    "                    idx=tokens_lower.index(label)\n",
    "                label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "                line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                if label not in line_ngram:\n",
    "                    label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                    line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram:\n",
    "                        label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                        line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                        if label not in line_ngram:\n",
    "                            label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                            line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                idx=line_ngram.index(label)\n",
    "                attribute_examples[i].append(line)\n",
    "                attribute_labels[i].append([idx+j for j in range(len(label))])\n",
    "                attr_label=label\n",
    "                \n",
    "    if not neutral_flag and stereotype_set&token_set:\n",
    "        line=orig_line\n",
    "        line=tokenizer.encode(line, add_special_tokens=False)\n",
    "        neutr_labels=stereotype_set&token_set\n",
    "        for label in list(neutr_labels):\n",
    "            stereotype_attr_labels.append(attr_label)\n",
    "            idx=tokens_lower.index(label)\n",
    "            label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "            line_ngram_neutral=list(nltk.ngrams(line, len(label)))\n",
    "            if label not in line_ngram_neutral:\n",
    "                label = tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "                if label not in line_ngram_neutral:\n",
    "                    label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                    line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram_neutral:\n",
    "                        label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                        line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "            try:\n",
    "                idx=line_ngram_neutral.index(label)\n",
    "                neutral_examples.append(line)\n",
    "                neutral_labels.append(label)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            #idx=line_ngram_neutral.index(label)\n",
    "            #neutral_examples.append(line)\n",
    "            #neutral_labels.append(label)\n",
    "            #neutral_labels.append([idx+i for i in range(len(label))])\n",
    "                    \n",
    "                \n",
    "print('neutral:', len(neutral_examples))\n",
    "for i, examples in enumerate(attribute_examples):\n",
    "    print(f'attributes{i}:', len(examples))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1bdffdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neutral_examples)):\n",
    "    neutral_examples[i]=tokenizer.decode(neutral_examples[i])\n",
    "    neutral_labels[i]=tokenizer.decode(neutral_labels[i])\n",
    "    stereotype_attr_labels[i]=tokenizer.decode(stereotype_attr_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aef022f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereotype_data=[]\n",
    "for i in range(len(neutral_examples)):\n",
    "    this_line=defaultdict()\n",
    "    if stereotype_attr_labels[i]!='':\n",
    "        this_line['g']=data_w_labels[stereotype_attr_labels[i]]\n",
    "        this_line['s']=neutral_labels[i]\n",
    "        this_line['text']=neutral_examples[i]\n",
    "        stereotype_data.append(this_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "428b582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereotype_data_train=stereotype_data[:round(len(stereotype_data)*train_dev_split)]\n",
    "stereotype_data_dev=stereotype_data[round(len(stereotype_data)*train_dev_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c43fc9fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data_for_inlp\"):\n",
    "    os.makedirs(\"data_for_inlp\")\n",
    "if not os.path.exists(\"data_for_inlp/{}\".format(this_model_type)):\n",
    "    os.makedirs(\"data_for_inlp/{}\".format(this_model_type))\n",
    "\n",
    "with open('./data_for_inlp/{}/{}.pickle'.format(this_model_type, output_file_train), 'wb') as f:\n",
    "    pickle.dump(stereotype_data_train, f)\n",
    "\n",
    "if train_dev_split<1:\n",
    "    with open('./data_for_inlp/{}/{}.pickle'.format(this_model_type, output_file_dev), 'wb') as f:\n",
    "        pickle.dump(stereotype_data_dev, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}