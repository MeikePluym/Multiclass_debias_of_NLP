{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e981d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import regex as re\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import *\n",
    "import os.path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "476b1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_model_type='dbert' #from ['bert', 'roberta', 'albert', 'dbert', 'electra', 'gpt2']\n",
    "this_block_size=128\n",
    "attributes=['./attribute_target_words/attributes/ethnicities_colour.txt']\n",
    "stereotypes_file='./attribute_target_words/targets/physical_adjectives.txt'  #not using stereotypes; define ''\n",
    "\n",
    "attr_w_labels='ethnicities_w_label.txt'\n",
    "\n",
    "output_file_train='ethnicities_phys_data_for_inlp_train'\n",
    "train_dev_split=0.9\n",
    "output_file_dev='ethnicities_phys_data_for_inlp_dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "text_file_w_labels='./attribute_target_words/with_label/{}'.format(attr_w_labels)\n",
    "data_w_labels=defaultdict()\n",
    "\n",
    "data=[word.strip().split() for word in open(text_file_w_labels)]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data_w_labels[data[i][1]]=data[i][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c72fd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('news_commentary_v15.en'):\n",
    "    f_in=gzip.open('news-commentary-v15.en.gz') #download from website; or use curl\n",
    "    f_out=open('news_commentary_v15.en', 'wb')\n",
    "    f_out.writelines(f_in)\n",
    "    f_out.close()\n",
    "    f_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3b1bcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_commentary_v15.en', 'r', encoding='utf-8') as f:\n",
    "    lines=f.readlines()\n",
    "\n",
    "data=[l.strip() for l in lines]\n",
    "\n",
    "if stereotypes_file:\n",
    "    stereotypes=[word.strip() for word in open(stereotypes_file)]\n",
    "    stereotype_set=set(stereotypes)\n",
    "\n",
    "pat=re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "attributes_l=[]\n",
    "all_attributes_set=set()\n",
    "for attribute in attributes:\n",
    "    l=[word.strip() for word in open(attribute)]\n",
    "    attributes_l.append(set(l))\n",
    "    all_attributes_set |= set(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e1355729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transformer(model_type):\n",
    "    if model_type=='bert':\n",
    "        pretrained_weights='bert-base-uncased'\n",
    "        model=BertModel.from_pretrained(pretrained_weights, output_hidden_states=True)\n",
    "        tokenizer=BertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='roberta':\n",
    "        pretrained_weights='roberta-base'\n",
    "        model=RobertaModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=RobertaTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='albert':\n",
    "        pretrained_weights='albert-base-v2'\n",
    "        model=AlbertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=AlbertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='dbert':\n",
    "        pretrained_weights='distilbert-base-uncased'\n",
    "        model=DistilBertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xlnet':\n",
    "        pretrained_weights='xlnet-base-cased'\n",
    "        model=XLNetModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=XLNetTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='electra':\n",
    "        pretrained_weights='google/electra-small-discriminator'\n",
    "        model=ElectraModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=ElectraTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt':\n",
    "        pretrained_weights='openai-gpt'\n",
    "        model=OpenAIGPTModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=OpenAIGPTTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt2':\n",
    "        pretrained_weights='gpt2'\n",
    "        model=GPT2Model.from_pretrained(pretrained_weights)\n",
    "        tokenizer=GPT2Tokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xl':\n",
    "        pretrained_weights='transfo-xl-wt103'\n",
    "        model=TransfoXLModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=TransfoXLTokenizer.from_pretrained(pretrained_weights)\n",
    "    return model, tokenizer\n",
    "\n",
    "def encode_to_is(tokenizer, the_data, add_special_tokens):\n",
    "    if type(the_data)==list:\n",
    "        data=[tuple(tokenizer.encode(sentence, add_special_tokens=add_special_tokens)) for sentence in the_data]\n",
    "    elif type(the_data)==dict:\n",
    "        data={tuple(tokenizer.encode(key, add_special_tokens=add_special_tokens)): tokenizer.encode(value, add_special_tokens=add_special_tokens) for key, value in the_data.items()}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "763bcc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer=prepare_transformer(this_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2350b38c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608912/608912 [00:21<00:00, 28745.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral: 76\n",
      "attributes0: 3594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if stereotypes_file:\n",
    "    tok_stereotypes=encode_to_is(tokenizer, stereotypes, add_special_tokens=False)\n",
    "\n",
    "neutral_examples=[]\n",
    "neutral_labels=[]\n",
    "stereotype_attr_labels=[]\n",
    "attribute_examples=[[] for _ in range(len(attributes_l))]\n",
    "attribute_labels=[[] for _ in range(len(attributes_l))]\n",
    "\n",
    "\n",
    "for line in tqdm(data):\n",
    "    neutral_flag=True\n",
    "    line=line.strip()\n",
    "    if len(line)<1:\n",
    "        continue\n",
    "    length=len(line.split())\n",
    "    if length>this_block_size or length<=1:\n",
    "        continue\n",
    "    tokens_orig=[token.strip() for token in re.findall(pat, line)]\n",
    "    tokens_lower=[token.lower() for token in tokens_orig]\n",
    "    token_set=set(tokens_lower)\n",
    "    \n",
    "    attribute_other_l=[]\n",
    "    for i, _ in enumerate(attributes_l):\n",
    "        a_set=set()\n",
    "        for j, attribute in enumerate(attributes_l):\n",
    "            if i!=j:\n",
    "                a_set |= attribute\n",
    "        attribute_other_l.append(a_set)\n",
    "    \n",
    "    for i, (attribute_set, other_set) in enumerate(zip(attributes_l, attribute_other_l)):\n",
    "        # & is bitwise AND operator\n",
    "        if attribute_set & token_set: #if a gender attribute is in the data line; classify the line as not neutral; and set the attribute to be the label\n",
    "            neutral_flag=False\n",
    "            if not other_set&token_set:\n",
    "                orig_line=line\n",
    "                line=tokenizer.encode(line, add_special_tokens=True)\n",
    "                labels=attribute_set & token_set\n",
    "                for label in list(labels):\n",
    "                    idx=tokens_lower.index(label)\n",
    "                label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "                line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                if label not in line_ngram:\n",
    "                    label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                    line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram:\n",
    "                        label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                        line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                        if label not in line_ngram:\n",
    "                            label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                            line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                idx=line_ngram.index(label)\n",
    "                attribute_examples[i].append(line)\n",
    "                attribute_labels[i].append([idx+j for j in range(len(label))])\n",
    "                attr_label=label\n",
    "                \n",
    "    if not neutral_flag and stereotype_set&token_set:\n",
    "        line=orig_line\n",
    "        line=tokenizer.encode(line, add_special_tokens=False)\n",
    "        neutr_labels=stereotype_set&token_set\n",
    "        for label in list(neutr_labels):\n",
    "            stereotype_attr_labels.append(attr_label)\n",
    "            idx=tokens_lower.index(label)\n",
    "            label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "            line_ngram_neutral=list(nltk.ngrams(line, len(label)))\n",
    "            if label not in line_ngram_neutral:\n",
    "                label = tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "                if label not in line_ngram_neutral:\n",
    "                    label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                    line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram_neutral:\n",
    "                        label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                        line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "            idx=line_ngram_neutral.index(label)\n",
    "            neutral_examples.append(line)\n",
    "            neutral_labels.append(label)\n",
    "            #neutral_labels.append([idx+i for i in range(len(label))])\n",
    "                    \n",
    "                \n",
    "print('neutral:', len(neutral_examples))\n",
    "for i, examples in enumerate(attribute_examples):\n",
    "    print(f'attributes{i}:', len(examples))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1bdffdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neutral_examples)):\n",
    "    neutral_examples[i]=tokenizer.decode(neutral_examples[i])\n",
    "    neutral_labels[i]=tokenizer.decode(neutral_labels[i])\n",
    "    stereotype_attr_labels[i]=tokenizer.decode(stereotype_attr_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "37968f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aef022f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereotype_data=[]\n",
    "for i in range(len(neutral_examples)):\n",
    "    this_line=defaultdict()\n",
    "    this_line['g']=data_w_labels[stereotype_attr_labels[i]]\n",
    "    this_line['s']=neutral_labels[i]\n",
    "    this_line['text']=neutral_examples[i]\n",
    "    stereotype_data.append(this_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "428b582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereotype_data_train=stereotype_data[:round(len(stereotype_data)*train_dev_split)]\n",
    "stereotype_data_dev=stereotype_data[round(len(stereotype_data)*train_dev_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c43fc9fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data_for_inlp\"):\n",
    "    os.makedirs(\"data_for_inlp\")\n",
    "if not os.path.exists(\"data_for_inlp/{}\".format(this_model_type)):\n",
    "    os.makedirs(\"data_for_inlp/{}\".format(this_model_type))\n",
    "\n",
    "with open('./data_for_inlp/{}/{}.pickle'.format(this_model_type, output_file_train), 'wb') as f:\n",
    "    pickle.dump(stereotype_data_train, f)\n",
    "\n",
    "if train_dev_split<1:\n",
    "    with open('./data_for_inlp/{}/{}.pickle'.format(this_model_type, output_file_dev), 'wb') as f:\n",
    "        pickle.dump(stereotype_data_dev, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}