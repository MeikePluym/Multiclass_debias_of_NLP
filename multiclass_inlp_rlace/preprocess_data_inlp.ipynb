{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e981d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import regex as re\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import *\n",
    "import os.path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "476b1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_model_type='bert' #from ['bert', 'roberta', 'albert', 'dbert', 'electra', 'gpt2']\n",
    "this_block_size=128\n",
    "attributes=['./attribute_target_words/attributes/gender.txt']\n",
    "stereotypes_file='./attribute_target_words/targets/gender_stereotype.txt'  #not using stereotypes; define ''\n",
    "\n",
    "attr_w_labels='gender_w_label.txt'\n",
    "\n",
    "output_file_train='gender_data_for_inlp_train'\n",
    "train_dev_split=0.9\n",
    "output_file_dev='gender_data_for_inlp_dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "text_file_w_labels='./attribute_target_words/with_label/{}'.format(attr_w_labels)\n",
    "data_w_labels=defaultdict()\n",
    "\n",
    "data=[word.strip().split() for word in open(text_file_w_labels)]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data_w_labels[data[i][1]]=data[i][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c72fd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('news_commentary_v15.en'):\n",
    "    f_in=gzip.open('news-commentary-v15.en.gz') #download from website; or use curl\n",
    "    f_out=open('news_commentary_v15.en', 'wb')\n",
    "    f_out.writelines(f_in)\n",
    "    f_out.close()\n",
    "    f_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b1bcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_commentary_v15.en', 'r', encoding='utf-8') as f:\n",
    "    lines=f.readlines()\n",
    "\n",
    "data=[l.strip() for l in lines]\n",
    "\n",
    "if stereotypes_file:\n",
    "    stereotypes=[word.strip() for word in open(stereotypes_file)]\n",
    "    stereotype_set=set(stereotypes)\n",
    "\n",
    "pat=re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "attributes_l=[]\n",
    "all_attributes_set=set()\n",
    "for attribute in attributes:\n",
    "    l=[word.strip() for word in open(attribute)]\n",
    "    attributes_l.append(set(l))\n",
    "    all_attributes_set |= set(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1355729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transformer(model_type):\n",
    "    if model_type=='bert':\n",
    "        pretrained_weights='bert-base-uncased'\n",
    "        model=BertModel.from_pretrained(pretrained_weights, output_hidden_states=True)\n",
    "        tokenizer=BertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='roberta':\n",
    "        pretrained_weights='roberta-base'\n",
    "        model=RobertaModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=RobertaTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='albert':\n",
    "        pretrained_weights='albert-base-v2'\n",
    "        model=AlbertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=AlbertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='dbert':\n",
    "        pretrained_weights='distilbert-base-uncased'\n",
    "        model=DistilBertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xlnet':\n",
    "        pretrained_weights='xlnet-base-cased'\n",
    "        model=XLNetModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=XLNetTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='electra':\n",
    "        pretrained_weights='google/electra-small-discriminator'\n",
    "        model=ElectraModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=ElectraTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt':\n",
    "        pretrained_weights='openai-gpt'\n",
    "        model=OpenAIGPTModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=OpenAIGPTTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt2':\n",
    "        pretrained_weights='gpt2'\n",
    "        model=GPT2Model.from_pretrained(pretrained_weights)\n",
    "        tokenizer=GPT2Tokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xl':\n",
    "        pretrained_weights='transfo-xl-wt103'\n",
    "        model=TransfoXLModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=TransfoXLTokenizer.from_pretrained(pretrained_weights)\n",
    "    return model, tokenizer\n",
    "\n",
    "def encode_to_is(tokenizer, the_data, add_special_tokens):\n",
    "    if type(the_data)==list:\n",
    "        data=[tuple(tokenizer.encode(sentence, add_special_tokens=add_special_tokens)) for sentence in the_data]\n",
    "    elif type(the_data)==dict:\n",
    "        data={tuple(tokenizer.encode(key, add_special_tokens=add_special_tokens)): tokenizer.encode(value, add_special_tokens=add_special_tokens) for key, value in the_data.items()}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "763bcc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer=prepare_transformer(this_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2350b38c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608912/608912 [01:06<00:00, 9162.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral: 9974\n",
      "attributes0: 56800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if stereotypes_file:\n",
    "    tok_stereotypes=encode_to_is(tokenizer, stereotypes, add_special_tokens=False)\n",
    "\n",
    "neutral_examples=[]\n",
    "neutral_labels=[]\n",
    "stereotype_attr_labels=[]\n",
    "attribute_examples=[[] for _ in range(len(attributes_l))]\n",
    "attribute_labels=[[] for _ in range(len(attributes_l))]\n",
    "\n",
    "\n",
    "for line in tqdm(data):\n",
    "    neutral_flag=True\n",
    "    line=line.strip()\n",
    "    if len(line)<1:\n",
    "        continue\n",
    "    length=len(line.split())\n",
    "    if length>this_block_size or length<=1:\n",
    "        continue\n",
    "    tokens_orig=[token.strip() for token in re.findall(pat, line)]\n",
    "    tokens_lower=[token.lower() for token in tokens_orig]\n",
    "    token_set=set(tokens_lower)\n",
    "    \n",
    "    attribute_other_l=[]\n",
    "    for i, _ in enumerate(attributes_l):\n",
    "        a_set=set()\n",
    "        for j, attribute in enumerate(attributes_l):\n",
    "            if i!=j:\n",
    "                a_set |= attribute\n",
    "        attribute_other_l.append(a_set)\n",
    "    \n",
    "    for i, (attribute_set, other_set) in enumerate(zip(attributes_l, attribute_other_l)):\n",
    "        # & is bitwise AND operator\n",
    "        if attribute_set & token_set: #if a gender attribute is in the data line; classify the line as not neutral; and set the attribute to be the label\n",
    "            neutral_flag=False\n",
    "            if not other_set&token_set:\n",
    "                orig_line=line\n",
    "                line=tokenizer.encode(line, add_special_tokens=True)\n",
    "                labels=attribute_set & token_set\n",
    "                for label in list(labels):\n",
    "                    idx=tokens_lower.index(label)\n",
    "                label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "                line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                if label not in line_ngram:\n",
    "                    label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                    line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram:\n",
    "                        label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                        line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                        if label not in line_ngram:\n",
    "                            label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                            line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                idx=line_ngram.index(label)\n",
    "                attribute_examples[i].append(line)\n",
    "                attribute_labels[i].append([idx+j for j in range(len(label))])\n",
    "                attr_label=label\n",
    "                \n",
    "    if not neutral_flag and stereotype_set&token_set:\n",
    "        line=orig_line\n",
    "        line=tokenizer.encode(line, add_special_tokens=False)\n",
    "        neutr_labels=stereotype_set&token_set\n",
    "        for label in list(neutr_labels):\n",
    "            stereotype_attr_labels.append(attr_label)\n",
    "            idx=tokens_lower.index(label)\n",
    "            label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "            line_ngram_neutral=list(nltk.ngrams(line, len(label)))\n",
    "            if label not in line_ngram_neutral:\n",
    "                label = tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "                if label not in line_ngram_neutral:\n",
    "                    label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                    line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram_neutral:\n",
    "                        label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                        line_ngram_neutral = list(nltk.ngrams(line, len(label)))\n",
    "            idx=line_ngram_neutral.index(label)\n",
    "            neutral_examples.append(line)\n",
    "            neutral_labels.append(label)\n",
    "            #neutral_labels.append([idx+i for i in range(len(label))])\n",
    "                    \n",
    "                \n",
    "print('neutral:', len(neutral_examples))\n",
    "for i, examples in enumerate(attribute_examples):\n",
    "    print(f'attributes{i}:', len(examples))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bdffdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neutral_examples)):\n",
    "    neutral_examples[i]=tokenizer.decode(neutral_examples[i])\n",
    "    neutral_labels[i]=tokenizer.decode(neutral_labels[i])\n",
    "    stereotype_attr_labels[i]=tokenizer.decode(stereotype_attr_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37968f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aef022f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereotype_data=[]\n",
    "for i in range(len(neutral_examples)):\n",
    "    this_line=defaultdict()\n",
    "    this_line['g']=data_w_labels[stereotype_attr_labels[i]]\n",
    "    this_line['s']=neutral_labels[i]\n",
    "    this_line['text']=neutral_examples[i]\n",
    "    stereotype_data.append(this_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "428b582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereotype_data_train=stereotype_data[:round(len(stereotype_data)*train_dev_split)]\n",
    "stereotype_data_dev=stereotype_data[round(len(stereotype_data)*train_dev_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c43fc9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data_for_inlp\"):\n",
    "    os.makedirs(\"data_for_inlp\")\n",
    "\n",
    "with open('./data_for_inlp/{}.pickle'.format(output_file_train), 'wb') as f:\n",
    "    pickle.dump(stereotype_data_train, f)\n",
    "\n",
    "if train_dev_split<1:\n",
    "    with open('./data_for_inlp/{}.pickle'.format(output_file_dev), 'wb') as f:\n",
    "        pickle.dump(stereotype_data_dev, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b502a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}