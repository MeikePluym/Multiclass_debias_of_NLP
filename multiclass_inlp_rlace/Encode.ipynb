{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9244d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import copy\n",
    "from typing import Dict, List, Tuple\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (MODEL_WITH_LM_HEAD_MAPPING, WEIGHTS_NAME, AdamW, AutoConfig, AutoModelWithLMHead, AutoTokenizer, \n",
    "                          PreTrainedModel, PreTrainedTokenizer, get_linear_schedule_with_warmup, BertModel, DistilBertModel, AlbertModel)\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorbord import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b359ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    \n",
    "    model_type='bert'\n",
    "    model_name_or_path=\"bert-base-uncased\"\n",
    "    cache_dir=None\n",
    "    run_id=1\n",
    "    device=-1\n",
    "    batch_size=32\n",
    "    \n",
    "    pickle_file_train='ethnicities_phys_data_for_inlp_train'\n",
    "    pickle_file_dev='ethnicities_phys_data_for_inlp_dev'\n",
    "    \n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62324800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(pickle_file):\n",
    "    with open(\"./data_for_inlp/{}/{}.pickle\".format(args.model_type, pickle_file), \"rb\") as f:\n",
    "        data=pickle.load(f)\n",
    "        txts=[d[\"text\"] for d in data]\n",
    "    return txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb48861",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device=\"cpu\" if args.device==-1 else \"cuda:{}\".format(args.device)\n",
    "\n",
    "config=AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
    "config.output_hidden_states='true'\n",
    "\n",
    "pretrained_weights=args.model_name_or_path\n",
    "\n",
    "if args.model_type=='bert':\n",
    "    model=BertModel.from_pretrained(pretrained_weights, config=config, cache_dir=args.cache_dir)\n",
    "elif args.model_type=='dbert':\n",
    "    model=DistilBertModel.from_pretrained(pretrained_weights, config=config, cache_dir=args.cache_dir)\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
    "\n",
    "block_size=tokenizer.model_max_length\n",
    "\n",
    "#model=BertModel.from_pretrained(args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config, cache_dir=args.cache_dir)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "rand_seed=args.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d7c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"encodings\"):\n",
    "    os.makedirs(\"encodings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79e7d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_train=load_pickle(args.pickle_file_train)\n",
    "data_file_dev=load_pickle(args.pickle_file_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c101f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(nlp_model, nlp_tokenizer, texts, args):\n",
    "    \n",
    "    all_H=[]\n",
    "    nlp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        print(\"Encoding...\")\n",
    "        batch_size=args.batch_size\n",
    "        pbar=tqdm(range(len(texts)), ascii=True)\n",
    "        \n",
    "        for i in range(0, len(texts)-batch_size, batch_size):\n",
    "            \n",
    "            batch_texts=texts[i: i+batch_size]\n",
    "            \n",
    "            batch_encoding=nlp_tokenizer.batch_encode_plus(batch_texts, padding=True, max_length=512, truncation=True)\n",
    "\n",
    "            if args.model_type=='bert':\n",
    "                input_ids, token_type_ids, attention_mask=batch_encoding[\"input_ids\"], batch_encoding[\"token_type_ids\"], batch_encoding[\"attention_mask\"]\n",
    "                input_ids=torch.tensor(input_ids).to(device)\n",
    "                token_type_ids=torch.tensor(token_type_ids).to(device)\n",
    "                attention_mask=torch.tensor(attention_mask).to(device)\n",
    "                H=nlp_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "            elif args.model_type=='dbert':\n",
    "                input_ids, attention_mask=batch_encoding[\"input_ids\"], batch_encoding[\"attention_mask\"]\n",
    "                input_ids=torch.tensor(input_ids).to(device)\n",
    "                attention_mask=torch.tensor(attention_mask).to(device)\n",
    "                #below is the equivalent to the pooler_ouput from BERT for the DistilBert model; a fixed-size vector representation of the entire input sequence\n",
    "                hiddens=nlp_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "                H=hiddens[:, 0, :]\n",
    "            \n",
    "            assert len(H.shape)==2\n",
    "            all_H.append(H.detach().cpu().numpy())\n",
    "            \n",
    "            pbar.update(batch_size)\n",
    "        \n",
    "        remaining=texts[(len(texts)//args.batch_size)*args.batch_size:]\n",
    "        if len(remaining)>0:\n",
    "            \n",
    "            batch_encoding=nlp_tokenizer.batch_encode_plus(remaining, padding=True, max_length=512, truncation=True)\n",
    "\n",
    "            if args.model_type=='bert':\n",
    "                input_ids, token_type_ids, attention_mask=batch_encoding[\"input_ids\"], batch_encoding[\"token_type_ids\"], batch_encoding[\"attention_mask\"]\n",
    "                input_ids=torch.tensor(input_ids).to(device)\n",
    "                token_type_ids=torch.tensor(token_type_ids).to(device)\n",
    "                attention_mask=torch.tensor(attention_mask).to(device)\n",
    "                H=nlp_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "            elif args.model_type=='dbert':\n",
    "                input_ids, attention_mask=batch_encoding[\"input_ids\"], batch_encoding[\"attention_mask\"]\n",
    "                input_ids=torch.tensor(input_ids).to(device)\n",
    "                attention_mask=torch.tensor(attention_mask).to(device)\n",
    "                #below is the equivalent to the pooler_ouput from BERT for the DistilBert model; a fixed-size vector representation of the entire input sequence\n",
    "                hiddens=nlp_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "                H=hiddens[:, 0, :]\n",
    "\n",
    "            #H=nlp_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "            \n",
    "            assert len(H.shape)==2\n",
    "            all_H.append(H.detach().cpu().numpy())\n",
    "        \n",
    "    H_np=np.concatenate(all_H)\n",
    "    assert len(H_np.shape)==2\n",
    "    assert len(H_np)==len(texts)\n",
    "    return H_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f73d5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|#########4| 64/68 [00:09<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "H_train=encode(model, tokenizer, data_file_train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd4d18ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "H_dev=encode(model, tokenizer, data_file_dev, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.80399084, -0.60692686, -0.89353573, ..., -0.656607  ,\n        -0.6405367 ,  0.63855124],\n       [-0.8549681 , -0.30960402, -0.16254   , ..., -0.37514246,\n        -0.5749633 ,  0.8057628 ],\n       [-0.8629364 , -0.39871433, -0.9563958 , ..., -0.9303482 ,\n        -0.70047444,  0.735096  ],\n       ...,\n       [-0.7412005 , -0.31360584, -0.77346873, ..., -0.62608105,\n        -0.5615992 ,  0.76570374],\n       [-0.77566355, -0.40765655, -0.8068566 , ..., -0.13706821,\n        -0.6080266 ,  0.5931715 ],\n       [-0.8787884 , -0.5682052 , -0.9553962 , ..., -0.73718697,\n        -0.6067792 ,  0.6446022 ]], dtype=float32)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bfe90b46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"encodings/{}\".format(args.model_type)):\n",
    "    os.makedirs(\"encodings/{}\".format(args.model_type))\n",
    "\n",
    "path_train=\"encodings/{}/{}.npy\".format(args.model_type, args.pickle_file_train)\n",
    "np.save(path_train, H_train)\n",
    "\n",
    "path_dev=\"encodings/{}/{}.npy\".format(args.model_type, args.pickle_file_dev)\n",
    "np.save(path_dev, H_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}