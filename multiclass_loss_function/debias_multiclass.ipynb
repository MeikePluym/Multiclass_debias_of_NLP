{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import copy\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (MODEL_WITH_LM_HEAD_MAPPING, WEIGHTS_NAME, AdamW, AutoConfig, AutoModelWithLMHead, AutoTokenizer, PreTrainedModel, PreTrainedTokenizer, get_linear_schedule_with_warmup)\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Args:\n",
    "    #model architecture to be trained/fine-tuned\n",
    "    model_type='bert'   #['bert', 'roberta', 'albert', 'dbert', 'electra', 'gpt2']\n",
    "    #weights for loss function\n",
    "    alpha=0.2\n",
    "    beta=1-alpha\n",
    "    #number of attributes in attribute dict; always 1 (?)\n",
    "    num_attributes=1\n",
    "    #keep training from checkpoint; train next epoch (fine-tuning). False if from scratch\n",
    "    checkpoint=False\n",
    "\n",
    "    #input data file\n",
    "    data_files=['./data/'+model_type+'/gender_stereotype_data.bin', './data/'+model_type+'/ethnicities_polarized_class_data.bin', './data/'+model_type+'/ethnicities_physical_data.bin', './data/'+model_type+'/religion_polarized_class_data.bin']\n",
    "    #data_file='./data/'+model_type+'/gender_stereotype_data.bin'\n",
    "    #data_file_ethnicities='./data/'+model_type+'/ethnicities_polarized_class_data.bin'\n",
    "\n",
    "    #output directory where model predictions/checkpoints will be written\n",
    "    output_dir='./debiased_models/'+model_type\n",
    "\n",
    "    #model checkpoint for weights initialization; None if you want to train model from scratch\n",
    "    if model_type=='bert':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/bert/debiased-checkpoint-best/'\n",
    "        else:\n",
    "            model_name_or_path='bert-base-uncased'\n",
    "    elif model_type=='roberta':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/roberta/debiased-checkpoint-best/'\n",
    "        else:\n",
    "            model_name_or_path='roberta-base'\n",
    "    elif model_type=='albert':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/albert/debiased-checkpoint-best/'\n",
    "        else:\n",
    "            model_name_or_path='albert-base-v2'\n",
    "    elif model_type=='dbert':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/dbert/debiased-checkpoint-best'\n",
    "        else:\n",
    "            model_name_or_path='distilbert-base-uncased'\n",
    "    elif model_type=='electra':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/electra/debiased-checkpoint-best'\n",
    "        else:\n",
    "            model_name_or_path='google/electra-small-discriminator'\n",
    "    elif model_type=='gpt2':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/gpt2/debiased-checkpoint-best'\n",
    "        else:\n",
    "            model_name_or_path='gpt2'\n",
    "\n",
    "    #optional pretrained config name/path if not same as model_name_or_path\n",
    "    config_name=''\n",
    "\n",
    "    #optional pretrained tokenizer name/path\n",
    "    tokenizer_name=''\n",
    "\n",
    "    #optional input evaluation data file\n",
    "    eval_data_file=None\n",
    "\n",
    "    #whether distinct lines of text in data are to be handled as distinct sequences\n",
    "    line_by_line=True\n",
    "\n",
    "    #whether to continue from latest checkpoint in output_dir\n",
    "    should_continue=False\n",
    "\n",
    "    #loss target; sentence or token\n",
    "    loss_target=\"sentence\"\n",
    "\n",
    "    #train with MLM loss instead of language modeling\n",
    "    mlm=False\n",
    "\n",
    "    #ratio of tokens to mask for MLM loss\n",
    "    mlm_probability=0.15\n",
    "\n",
    "    #optional directory to store pre-trained models\n",
    "    cache_dir=None\n",
    "\n",
    "    #optional input sequence length after tokenization; training data will be truncated. Default; max input length for single sentence inputs\n",
    "    block_size=128\n",
    "\n",
    "    #whether to run training\n",
    "    do_train=True\n",
    "    #whether to run eval\n",
    "    do_eval=True\n",
    "    #run eval during training at each logging step\n",
    "    evaluate_during_training=True\n",
    "\n",
    "    #batch size for train\n",
    "    per_gpu_train_batch_size=32\n",
    "    #bathc size for eval\n",
    "    per_gpu_eval_batch_size=32\n",
    "\n",
    "    #numb. of update steps to accumulate before performing backward/update pass\n",
    "    gradient_accumulation_steps=1\n",
    "    #initial learning rate for Adam\n",
    "    learning_rate=5e-5\n",
    "    #weight decay (if applied)\n",
    "    weight_decay=0.0\n",
    "    #epsilon for Adam optimizer\n",
    "    adam_epsilon= 1e-8\n",
    "    #max gradient norm.\n",
    "    max_grad_norm=1.0\n",
    "    #weighted loss\n",
    "    weighted_loss=[alpha, beta]\n",
    "    #['all', 'first', 'last'] to debias\n",
    "    debias_layer='all'\n",
    "    #number of training epochs\n",
    "    num_train_epochs=1\n",
    "    #if >0; set total number of training steps to perform\n",
    "    max_steps=-1\n",
    "    #linear warmup\n",
    "    warmup_steps=0\n",
    "    #square loss\n",
    "    square_loss=True\n",
    "    #token loss\n",
    "    token_loss=False\n",
    "    #log every x update steps\n",
    "    logging_steps=500\n",
    "    #save checkpoint every x update steps\n",
    "    save_steps=500\n",
    "    #limit total amount of checkpoints, delete older in output_dir\n",
    "    save_total_limit=None\n",
    "    #evaluate all checkpoints\n",
    "    eval_all_checkpoints=False\n",
    "    #avoid using CUDA when available\n",
    "    no_cuda=True\n",
    "    #overwrite content of output directory\n",
    "    overwrite_output_dir=True\n",
    "    #overwrite cached training and eval sets\n",
    "    overwrite_cache=False\n",
    "    #random seed for initialization\n",
    "    seed=42\n",
    "    #evaluation data set size\n",
    "    dev_data_size=1000\n",
    "    train_data_size=None\n",
    "\n",
    "    #whether to use 16-bit (mixed) precision instead of 32-bit\n",
    "    fp16=False\n",
    "    #Apex AMP opt level selected ['00', '01', '02', '03']\n",
    "    fp16_opt_level=\"01\"\n",
    "\n",
    "    tp=lambda x:list(x.split(','))\n",
    "    #exclusion list\n",
    "    exclusion_list=[]\n",
    "    #for distributed training\n",
    "    local_rank=-1\n",
    "    #for distant debugging\n",
    "    server_ip=\"\"\n",
    "    #for distant debugging\n",
    "    server_port=\"\"\n",
    "\n",
    "    n_gpu=0\n",
    "\n",
    "args=Args()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "logger=logging.getLogger(__name__)\n",
    "MODEL_CONFIG_CLASSES=list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES=tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
    "        assert os.path.isfile(file_path)\n",
    "\n",
    "        block_size=block_size-(tokenizer.model_max_length - tokenizer.max_model_input_sizes)\n",
    "\n",
    "        directory, filename=os.path.split(file_path)\n",
    "        cached_features_file=os.path.join(directory, args.model_type + \"_cached_lm_\"+str(block_size)+\"_\"+filename)\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples=pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "            self.examples=[]\n",
    "            with open(file_path, encoding=\"utf-8\") as f:\n",
    "                text=f.read()\n",
    "\n",
    "            tokenized_text=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "            for i in range(0, len(tokenized_text)-block_size+1, block_size):\n",
    "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dumpt(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, examples: list, labels: list):\n",
    "        self.examples=examples\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.labels:\n",
    "            return torch.tensor(self.examples[i], dtype=torch.long), torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        else:\n",
    "            return torch.tensor(self.examples[i], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def create_dataset(data, dataset):\n",
    "    d=dict()\n",
    "    for key in data['example'].keys():\n",
    "        if key not in data['label']:\n",
    "            d[key]=dataset(data['example'][key], None)\n",
    "        else:\n",
    "            d[key]=dataset(data['example'][key], data['label'][key])\n",
    "    return d\n",
    "\n",
    "def load_and_cache_examples(data, args, tokenizer):\n",
    "    if args.line_by_line:\n",
    "        train_dataset=create_dataset(data['train'], LineByLineTextDataset)\n",
    "        dev_dataset=create_dataset(data['dev'], LineByLineTextDataset)\n",
    "        return {'train': train_dataset, 'dev': dev_dataset}\n",
    "    #else:\n",
    "     #   return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
    "    #NB; WHAT IS FILE PATH HERE? WILL GIVE ERROR OF LINE_BY_LINE=FALSE\n",
    "\n",
    "def split_data(attributes_examples, attributes_labels, neutral_examples, neutral_labels, args):\n",
    "    data={'train': {'example': {}, 'label': {}}, 'dev': {'example': {}, 'label': {}}}\n",
    "    for i in range(len(attributes_examples)):\n",
    "        for j, (examples, labels) in enumerate(zip(attributes_examples[i], attributes_labels[i])):\n",
    "            idx_l=list(range(len(examples)))\n",
    "            data['train']['example'][f'attribute{i}'] = examples[args.dev_data_size:args.train_data_size]\n",
    "            data['train']['label'][f'attribute{i}'] = labels[args.dev_data_size:args.train_data_size]\n",
    "            data['dev']['example'][f'attribute{i}'] = examples[:args.dev_data_size]\n",
    "            data['dev']['label'][f'attribute{i}'] = labels[:args.dev_data_size]\n",
    "\n",
    "    for i, (neutral_examples, neutral_labels) in enumerate(zip(neutral_examples, neutral_labels)):\n",
    "        idx_l=list(range(len(neutral_examples)))\n",
    "        random.shuffle(idx_l)\n",
    "        neutral_examples=[neutral_examples[idx] for idx in idx_l]\n",
    "        data['train']['example'][f'neutral{i}']=neutral_examples[args.dev_data_size:args.train_data_size]\n",
    "        data['dev']['example'][f'neutral{i}']=neutral_examples[:args.dev_data_size]\n",
    "        if neutral_labels is not None:\n",
    "            neutral_labels=[neutral_labels[idx] for idx in idx_l]\n",
    "            data['train']['label'][f'neutral{i}'] = neutral_labels[args.dev_data_size:args.train_data_size]\n",
    "            data['dev']['label'][f'neutral{i}'] = neutral_labels[:args.dev_data_size]\n",
    "    return data\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "def create_dataloader(args, datasets, tokenizer, train=False):\n",
    "    def collate(batch: List[torch.Tensor]):\n",
    "        if type(batch[0])==tuple:\n",
    "            examples, labels=list(zip(*batch))\n",
    "            return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id), pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "        else:\n",
    "            return pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    dataloaders={}\n",
    "    example_num=0\n",
    "    data_distribution=[]\n",
    "\n",
    "    max_size = max([len(value) for key, value in datasets.items() if key != 'neutral'])\n",
    "    min_size = min([len(value) for key, value in datasets.items() if key != 'neutral'])\n",
    "\n",
    "    for key, dataset in datasets.items():\n",
    "        example_num+=len(dataset)\n",
    "        if train:\n",
    "            #CHECK PER GPU BATCH SIZE VS TRAIN BATCH SIZE\n",
    "            dataloaders[key]=iter(DataLoader(dataset, batch_size=args.per_gpu_train_batch_size, collate_fn=collate, shuffle=True))\n",
    "            data_distribution+=[key for _ in range(int(min_size/args.per_gpu_train_batch_size))]\n",
    "        else:\n",
    "            dataloaders[key]=iter(DataLoader(dataset, batch_size=args.per_gpu_eval_batch_size, collate_fn=collate, shuffle=True))\n",
    "            data_distribution+=[key for _ in range(int(min_size/args.per_gpu_eval_batch_size))]\n",
    "    return dataloaders, example_num, data_distribution\n",
    "\n",
    "def train(args, data,  datasets, model: PreTrainedModel, original_model, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer=SummaryWriter()\n",
    "\n",
    "    args.train_batch_size=args.per_gpu_train_batch_size*max(1, args.n_gpu)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    train_datasets=datasets['train']\n",
    "    dev_datasets=datasets['dev']\n",
    "\n",
    "    train_dataloaders, train_example_num, train_distribution=create_dataloader(args, train_datasets, tokenizer, train=True)\n",
    "    dev_dataloaders, dev_example_num, dev_distribution = create_dataloader(args, dev_datasets, tokenizer, train=False)\n",
    "\n",
    "    train_iter_num=sum([len(dataloader) for dataloader in train_dataloaders.values()])\n",
    "    dev_iter_num=sum([len(dataloader) for dataloader in dev_dataloaders.values()])\n",
    "\n",
    "    if args.max_steps>0:\n",
    "        t_total=args.max_steps\n",
    "        args.num_train_epochs=args.max_steps//(train_iter_num//args.gradient_accumulation_steps)+1\n",
    "    else:\n",
    "        t_total = train_iter_num // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model=model.module if hasattr(model, \"module\") else model\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    original_model = original_model.module if hasattr(original_model, \"module\") else original_model  # Take care of distributed/parallel training\n",
    "    original_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters=[\n",
    "        {\n",
    "            \"params\": [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        original_model = torch.nn.DataParallel(original_model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n",
    "        original_model = torch.nn.parallel.DistributedDataParallel(\n",
    "            original_model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n",
    "\n",
    "    #Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", train_example_num)\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step=0\n",
    "    epochs_trained=0\n",
    "    best_loss=float('inf')\n",
    "    best_step=0\n",
    "    steps_trained_in_current_epoch=0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (train_iter_num // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (train_iter_num // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    model.zero_grad()\n",
    "    original_model.zero_grad()\n",
    "    #train_iterator=trange(epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    train_iterator=range(epochs_trained, int(args.num_train_epochs))\n",
    "\n",
    "    def inner_product(x,y):\n",
    "        return torch.mean(torch.sum(y*x, 3))\n",
    "\n",
    "    def mean_square(x,y,idx):\n",
    "        return torch.mean(torch.mean((y-x)**2, idx))\n",
    "\n",
    "    def save_best_model(best_loss, best_step, dev_dataloaders):\n",
    "        if (args.local_rank == -1 and args.evaluate_during_training):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "            eval_loss = evaluate(model, attributes_hiddens, dev_dataloaders)\n",
    "            logger.info(\" global_step = %s, evaluate loss = %s\", global_step, eval_loss)\n",
    "            tb_writer.add_scalar(\"eval_loss\", eval_loss, global_step)\n",
    "        tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "\n",
    "        if eval_loss<best_loss:\n",
    "            best_loss = eval_loss\n",
    "            best_step = global_step\n",
    "            checkpoint_prefix = \"checkpoint\"\n",
    "            # Save model checkpoint\n",
    "            output_dir = os.path.join(args.output_dir, \"debiased-checkpoint-best\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            model_to_save = (\n",
    "                model.module if hasattr(model, \"module\") else model\n",
    "            )  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "        logger.info(\" best_step = %s, best loss = %s\", best_step, best_loss)\n",
    "\n",
    "        original_output_dir=os.path.join(args.output_dir, \"original-checkpoint-best\")\n",
    "        os.makedirs(original_output_dir, exist_ok=True)\n",
    "        original_model_to_save=(\n",
    "            original_model.module if hasattr(original_model, \"module\") else original_model\n",
    "        )\n",
    "        original_model_to_save.save_pretrained(original_output_dir)\n",
    "        tokenizer.save_pretrained(original_output_dir)\n",
    "\n",
    "        torch.save(args, os.path.join(original_output_dir, \"training_args.bin\"))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(original_output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(original_output_dir, \"scheduler.pt\"))\n",
    "        return best_loss, best_step\n",
    "\n",
    "    def get_hiddens_of_model(input):\n",
    "        model.zero_grad()\n",
    "        if args.model_type == 'roberta':\n",
    "            #_, _, hiddens = model.roberta(input)\n",
    "            hiddens=model.roberta(input).hidden_states\n",
    "        elif args.model_type == 'bert':\n",
    "            #changed\n",
    "            hiddens=model.bert(input).hidden_states\n",
    "        elif args.model_type == 'albert':\n",
    "            #_, _, hiddens = model.albert(input)\n",
    "            hiddens=model.albert(input).hidden_states\n",
    "        elif args.model_type == 'dbert':\n",
    "            #changed\n",
    "            #_, hiddens = model.distilbert(input)\n",
    "            hiddens=model.distilbert(input).hidden_states\n",
    "        elif args.model_type == 'electra':\n",
    "            _, hiddens = model.electra(input)\n",
    "        elif args.model_type == 'gpt2':\n",
    "            #_, _, hiddens = model.transformer(input)\n",
    "            hiddens=model.transformer(input).hidden_states\n",
    "        elif args.model_type == 'gpt':\n",
    "            _, hiddens = model.transformer(input)\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "    def attribute_vector_example():\n",
    "        attributes_hiddens = {f'attribute{i}': [] for i in range(len(args.data_files))}\n",
    "\n",
    "        dataloaders, _, distribution = create_dataloader(args, train_datasets, tokenizer, train=True)\n",
    "        for key in distribution:\n",
    "            if 'neutral' not in key:\n",
    "                inputs, labels = next(dataloaders[key])\n",
    "                inputs = inputs.to(args.device)\n",
    "                hiddens = get_hiddens_of_model(inputs)\n",
    "                hiddens = torch.stack(hiddens, 2)\n",
    "                if labels.size(1) > 1:\n",
    "                    onehot = torch.eye(hiddens.size(1))\n",
    "                    zeros = torch.zeros(1, onehot.size(0))\n",
    "                    onehot = torch.cat((zeros, onehot), 0)\n",
    "                    onehot = onehot[labels]\n",
    "                    onehot = torch.sum(onehot, 1)\n",
    "                    onehot = onehot.view(hiddens.size(0), -1, 1, 1)\n",
    "                else:\n",
    "                    onehot = torch.eye(hiddens.size(1))[labels].view(hiddens.size(0), -1, 1, 1)\n",
    "                onehot = onehot.to(args.device)\n",
    "                attributes_hiddens[key].append(torch.sum(hiddens * onehot, 1) / labels.size(1))\n",
    "\n",
    "        # neutral\n",
    "        attribute_size = len(args.data_files)\n",
    "        for i in range(attribute_size):\n",
    "            attributes_hiddens[f'attribute{i}'] = torch.mean(torch.cat(attributes_hiddens[f'attribute{i}'], 0), 0).detach().unsqueeze(0)\n",
    "\n",
    "        return attributes_hiddens\n",
    "\n",
    "    def forward(attributes_hiddens, dataloaders, key):\n",
    "        inputs = next(dataloaders[key])\n",
    "        if len(inputs) == 2:\n",
    "            inputs, labels = inputs\n",
    "            labels = labels.to(args.device)\n",
    "        else:\n",
    "            labels = None\n",
    "        inputs = inputs.to(args.device)\n",
    "        if args.model_type == 'roberta':\n",
    "            #final_layer_hiddens, first_token_hidden, all_layer_hiddens = model.roberta(inputs)\n",
    "            all_layer_hiddens=model.roberta(inputs).hidden_states\n",
    "            final_layer_hiddens=model.roberta(inputs).last_hidden_state\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    #final_layer_original_hiddens, _, all_layer_original_hiddens = original_model.roberta(inputs)\n",
    "                    all_layer_original_hiddens=original_model.roberta(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.roberta(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.lm_head(final_layer_hiddens)\n",
    "                    token_original = original_model.lm_head(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'bert':\n",
    "            all_layer_hiddens=model.bert(inputs).hidden_states\n",
    "            final_layer_hiddens=model.bert(inputs).last_hidden_state\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    all_layer_original_hiddens=original_model.bert(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.bert(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.cls(final_layer_hiddens)\n",
    "                    token_original = original_model.cls(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'albert':\n",
    "            #final_layer_hiddens, first_token_hidden, all_layer_hiddens = model.albert(inputs)\n",
    "            all_layer_hiddens=model.albert(inputs).hidden_states\n",
    "            final_layer_hiddens=model.albert(inputs).last_hidden_state\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    #final_layer_original_hiddens, _, all_layer_original_hiddens = original_model.albert(inputs)\n",
    "                    all_layer_original_hiddens=original_model.albert(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.albert(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.classifier(final_layer_hiddens)\n",
    "                    token_original = original_model.classifier(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'dbert':\n",
    "            #final_layer_hiddens, all_layer_hiddens = model.distilbert(inputs)\n",
    "            all_layer_hiddens=model.distilbert(inputs).hidden_states\n",
    "            final_layer_hiddens=model.distilbert(inputs).last_hidden_state\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    #final_layer_original_hiddens, all_layer_original_hiddens = original_model.distilbert(inputs)\n",
    "                    all_layer_original_hiddens=original_model.distilbert(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.distilbert(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.classifier(final_layer_hiddens)\n",
    "                    token_original = original_model.classifier(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'electra':\n",
    "            final_layer_hiddens, all_layer_hiddens = model.electra(inputs)\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                    final_layer_original_hiddens, all_layer_original_hiddens = original_model.electra(inputs)\n",
    "                if args.token_loss:\n",
    "                    hiddens = model.generator_predictions(final_layer_hiddens)\n",
    "                    token_predicts = model.generator_lm_head(hiddens)\n",
    "                    original_hiddens = original_model.generator_predictions(final_layer_original_hiddens)\n",
    "                    token_original = original_model.generator_lm_head(original_hiddens)\n",
    "        elif args.model_type == 'gpt2':\n",
    "            all_layer_hiddens=model.transformer(inputs).hidden_states\n",
    "            final_layer_hiddens=model.transformer(inputs).last_hidden_state\n",
    "            #final_layer_hiddens, first_token_hidden, all_layer_hiddens = model.transformer(inputs)\n",
    "            if 'neutral' not in key:\n",
    "                with torch.no_grad():\n",
    "                   #final_layer_original_hiddens, _, all_layer_original_hiddens = original_model.transformer(inputs)\n",
    "                    all_layer_original_hiddens=original_model.transformer(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.transformer(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.lm_head(final_layer_hiddens)\n",
    "                    token_original = original_model.lm_head(final_layer_original_hiddens)\n",
    "\n",
    "        all_layer_hiddens = torch.stack(all_layer_hiddens, 2)\n",
    "        if 'neutral' not in key:\n",
    "            all_original_hiddens =  torch.stack(all_layer_original_hiddens, 2)\n",
    "            all_original_hiddens = all_original_hiddens.detach()\n",
    "            if args.token_loss:\n",
    "                original_hiddens - original_hiddens.detach()\n",
    "                token_original = token_original.detach()\n",
    "        if args.debias_layer == 'all':\n",
    "            target_layer_hiddens = all_layer_hiddens\n",
    "            target_original_hiddens = all_layer_hiddens\n",
    "        else:\n",
    "            if args.debias_layer == 'first':\n",
    "                idx = 0\n",
    "            elif args.debias_layer == 'last':\n",
    "                idx = -1\n",
    "            target_layer_hiddens = all_layer_hiddens[:,:,idx]\n",
    "            target_layer_hiddens = target_layer_hiddens.unsqueeze(2)\n",
    "            if 'neutral' not in key:\n",
    "                target_original_hiddens = all_original_hiddens[:,:,idx]\n",
    "                target_original_hiddens = target_original_hiddens.unsqueeze(2)\n",
    "            else:\n",
    "                #attributes_hiddens = {key: value[:,idx,:].unsqueeze(2) for key, value in attributes_hiddens.items()}\n",
    "                attributes_hiddens = {key: value[:,idx,:].unsqueeze(1) for key, value in attributes_hiddens.items()}\n",
    "                #KLOPT DE UNSQUEEZE DIMENSIE HIER?\n",
    "\n",
    "        if args.loss_target == 'sentence' or labels is None:\n",
    "            attributes_hiddens = {key: value.unsqueeze(1) for key, value in attributes_hiddens.items()}\n",
    "        #elif args.loss_target == 'token' and key == 'neutral':\n",
    "        elif args.loss_target == 'token':\n",
    "            if labels.size(1) > 1:\n",
    "                onehot = torch.eye(target_layer_hiddens.size(1))\n",
    "                zeros = torch.zeros(1, onehot.size(0))\n",
    "                onehot = torch.cat((zeros, onehot), 0)\n",
    "                onehot = onehot[labels]\n",
    "                onehot = torch.sum(onehot, 1)\n",
    "                onehot = onehot.view(target_layer_hiddens.size(0), -1, 1, 1)\n",
    "            else:\n",
    "                onehot = torch.eye(target_layer_hiddens.size(1))[labels].view(target_layer_hiddens.size(0), -1, 1, 1)\n",
    "            onehot = onehot.to(args.device)\n",
    "            target_layer_hiddens = torch.sum(target_layer_hiddens * onehot, 1).unsqueeze(1) / labels.size(1)\n",
    "            if 'neutral' not in key:\n",
    "                target_original_hiddens = torch.sum(target_original_hiddens * onehot, 1).unsqueeze(1) / labels.size(1)\n",
    "                #KLOPT UNSQUEEZE HIER?\n",
    "            else:\n",
    "                attributes_hiddens = {key: value.expand(target_layer_hiddens.size(0), 1, value.size(1),value.size(2)) for key, value in attributes_hiddens.items()}\n",
    "\n",
    "        if 'neutral' in key: #VERANDER ALS MEER CLASSES; kan dit gewoon universal maken met; for i in len(args.data_files)\n",
    "            loss = 0\n",
    "            if key=='neutral0':\n",
    "                tmp_loss=criterion_ip(target_layer_hiddens, attributes_hiddens['attribute0'])\n",
    "                if args.square_loss:\n",
    "                    tmp_loss = tmp_loss ** 2\n",
    "                tmp_loss *= alpha\n",
    "                loss += tmp_loss\n",
    "            elif key=='neutral1':\n",
    "                tmp_loss=criterion_ip(target_layer_hiddens, attributes_hiddens['attribute1'])\n",
    "                if args.square_loss:\n",
    "                    tmp_loss = tmp_loss ** 2\n",
    "                tmp_loss *= alpha\n",
    "                loss += tmp_loss\n",
    "            elif key=='neutral2':\n",
    "                tmp_loss=criterion_ip(target_layer_hiddens, attributes_hiddens['attribute2'])\n",
    "                if args.square_loss:\n",
    "                    tmp_loss = tmp_loss ** 2\n",
    "                tmp_loss *= alpha\n",
    "                loss += tmp_loss\n",
    "            elif key=='neutral3':\n",
    "                tmp_loss=criterion_ip(target_layer_hiddens, attributes_hiddens['attribute3'])\n",
    "                if args.square_loss:\n",
    "                    tmp_loss = tmp_loss ** 2\n",
    "                tmp_loss *= alpha\n",
    "                loss += tmp_loss\n",
    "        else:\n",
    "            #loss = criterion_ms(target_layer_hiddens, target_original_hiddens)\n",
    "            loss = criterion_ms(all_layer_hiddens, all_original_hiddens, 3)\n",
    "            if args.token_loss:\n",
    "                loss += criterion_ms(token_predicts, token_original, 2)\n",
    "                #loss += criterion_ms(hiddens, original_hiddens, 2)\n",
    "            loss *= beta\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate(model, attributes_hiddens, dev_dataloaders, prefix=\"\"):\n",
    "        # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "        eval_output_dir = args.output_dir\n",
    "\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "        args.eval_batch_size=args.per_gpu_eval_batch_size*max(1,args.n_gpu)\n",
    "        # multi-gpu evaluate\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", dev_example_num)\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        for key in tqdm(dev_distribution):\n",
    "            with torch.no_grad():\n",
    "                loss = forward(attributes_hiddens, dev_dataloaders, key)\n",
    "\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                model.zero_grad()\n",
    "                original_model.zero_grad()\n",
    "\n",
    "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "            logger.info(\"  Loss = %s\", eval_loss)\n",
    "            writer.write(\"Loss = %s\\n\" % (eval_loss))\n",
    "\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    criterion_ms=mean_square\n",
    "    criterion_ip=inner_product\n",
    "    original_model.eval()\n",
    "\n",
    "    alpha, beta=args.weighted_loss\n",
    "    alpha=float(alpha)\n",
    "    beta=float(beta)\n",
    "\n",
    "    train_loss=0.0\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        random.shuffle(train_distribution)\n",
    "        epoch_iterator = tqdm(train_distribution, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            attributes_hiddens = attribute_vector_example()\n",
    "\n",
    "        for step, key in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            loss = forward(attributes_hiddens, train_dataloaders, key)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                original_model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    logger.info(\" global_step = %s, train loss = %s\", global_step, train_loss)\n",
    "                    train_loss = 0.0\n",
    "                    # Log metrics\n",
    "                    best_loss, best_step = save_best_model(best_loss, best_step, dev_dataloaders)\n",
    "                    dev_dataloaders, dev_example_num, dev_distribution = create_dataloader(args, dev_datasets, tokenizer, train=False)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "            train_dataloaders, train_example_num, train_distribution = create_dataloader(args, train_datasets, tokenizer, train=True)\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    dev_dataloaders, dev_example_num, dev_distribution = create_dataloader(args, dev_datasets, tokenizer, train=False)\n",
    "    best_loss, best_step = save_best_model(best_loss, best_step, dev_dataloaders)\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/22/2023 10:09:29 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "02/22/2023 10:09:37 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x000001A8A8096490>\n",
      "02/22/2023 10:09:42 - INFO - __main__ -   ***** Running training *****\n",
      "02/22/2023 10:09:42 - INFO - __main__ -     Num examples = 178805\n",
      "02/22/2023 10:09:42 - INFO - __main__ -     Num Epochs = 1\n",
      "02/22/2023 10:09:42 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
      "02/22/2023 10:09:42 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "02/22/2023 10:09:42 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "02/22/2023 10:09:42 - INFO - __main__ -     Total optimization steps = 5591\n",
      "Iteration:  79%|███████▉  | 499/632 [1:51:11<33:43, 15.21s/it]  02/22/2023 12:01:04 - INFO - __main__ -    global_step = 500, train loss = 5573.607160063766\n",
      "02/22/2023 12:01:04 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "02/22/2023 12:01:04 - INFO - __main__ -     Num examples = 8000\n",
      "02/22/2023 12:01:04 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "  0%|          | 0/248 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/248 [00:07<32:41,  7.94s/it]\u001B[A\n",
      "  1%|          | 2/248 [00:17<36:40,  8.94s/it]\u001B[A\n",
      "  1%|          | 3/248 [00:31<45:36, 11.17s/it]\u001B[A\n",
      "  2%|▏         | 4/248 [00:40<42:01, 10.33s/it]\u001B[A\n",
      "  2%|▏         | 5/248 [00:49<40:15,  9.94s/it]\u001B[A\n",
      "  2%|▏         | 6/248 [00:56<36:26,  9.04s/it]\u001B[A\n",
      "  3%|▎         | 7/248 [01:05<35:02,  8.72s/it]\u001B[A\n",
      "  3%|▎         | 8/248 [01:13<34:43,  8.68s/it]\u001B[A\n",
      "  4%|▎         | 9/248 [01:23<36:12,  9.09s/it]\u001B[A\n",
      "  4%|▍         | 10/248 [01:30<33:27,  8.44s/it]\u001B[A\n",
      "  4%|▍         | 11/248 [01:38<32:11,  8.15s/it]\u001B[A\n",
      "  5%|▍         | 12/248 [01:45<31:25,  7.99s/it]\u001B[A\n",
      "  5%|▌         | 13/248 [01:54<31:51,  8.14s/it]\u001B[A\n",
      "  6%|▌         | 14/248 [02:06<37:01,  9.50s/it]\u001B[A\n",
      "  6%|▌         | 15/248 [02:13<33:56,  8.74s/it]\u001B[A\n",
      "  6%|▋         | 16/248 [02:21<32:19,  8.36s/it]\u001B[A\n",
      "  7%|▋         | 17/248 [02:28<31:22,  8.15s/it]\u001B[A\n",
      "  7%|▋         | 18/248 [02:37<31:38,  8.26s/it]\u001B[A\n",
      "  8%|▊         | 19/248 [02:53<39:54, 10.45s/it]\u001B[A\n",
      "  8%|▊         | 20/248 [03:03<40:07, 10.56s/it]\u001B[A\n",
      "  8%|▊         | 21/248 [03:12<38:15, 10.11s/it]\u001B[A\n",
      "  9%|▉         | 22/248 [03:19<34:20,  9.12s/it]\u001B[A\n",
      "  9%|▉         | 23/248 [03:27<32:16,  8.61s/it]\u001B[A\n",
      " 10%|▉         | 24/248 [03:34<31:08,  8.34s/it]\u001B[A\n",
      " 10%|█         | 25/248 [03:44<32:21,  8.71s/it]\u001B[A\n",
      " 10%|█         | 26/248 [03:55<34:31,  9.33s/it]\u001B[A\n",
      " 11%|█         | 27/248 [04:02<31:39,  8.60s/it]\u001B[A\n",
      " 11%|█▏        | 28/248 [04:09<30:18,  8.27s/it]\u001B[A\n",
      " 12%|█▏        | 29/248 [04:17<29:33,  8.10s/it]\u001B[A\n",
      " 12%|█▏        | 30/248 [04:25<29:41,  8.17s/it]\u001B[A\n",
      " 12%|█▎        | 31/248 [04:33<29:24,  8.13s/it]\u001B[A\n",
      " 13%|█▎        | 32/248 [04:41<28:42,  7.97s/it]\u001B[A\n",
      " 13%|█▎        | 33/248 [04:49<29:17,  8.17s/it]\u001B[A\n",
      " 14%|█▎        | 34/248 [04:57<28:04,  7.87s/it]\u001B[A\n",
      " 14%|█▍        | 35/248 [05:07<30:23,  8.56s/it]\u001B[A\n",
      " 15%|█▍        | 36/248 [05:16<30:53,  8.74s/it]\u001B[A\n",
      " 15%|█▍        | 37/248 [05:26<31:55,  9.08s/it]\u001B[A\n",
      " 15%|█▌        | 38/248 [05:34<30:55,  8.84s/it]\u001B[A\n",
      " 16%|█▌        | 39/248 [05:44<31:50,  9.14s/it]\u001B[A\n",
      " 16%|█▌        | 40/248 [05:52<30:41,  8.85s/it]\u001B[A\n",
      " 17%|█▋        | 41/248 [06:00<30:01,  8.70s/it]\u001B[A\n",
      " 17%|█▋        | 42/248 [06:10<31:08,  9.07s/it]\u001B[A\n",
      " 17%|█▋        | 43/248 [06:19<30:21,  8.88s/it]\u001B[A\n",
      " 18%|█▊        | 44/248 [06:27<29:43,  8.74s/it]\u001B[A\n",
      " 18%|█▊        | 45/248 [06:34<27:35,  8.16s/it]\u001B[A\n",
      " 19%|█▊        | 46/248 [06:42<26:49,  7.97s/it]\u001B[A\n",
      " 19%|█▉        | 47/248 [06:52<29:18,  8.75s/it]\u001B[A\n",
      " 19%|█▉        | 48/248 [06:59<27:33,  8.27s/it]\u001B[A\n",
      " 20%|█▉        | 49/248 [07:09<28:27,  8.58s/it]\u001B[A\n",
      " 20%|██        | 50/248 [07:18<29:00,  8.79s/it]\u001B[A\n",
      " 21%|██        | 51/248 [07:31<33:04, 10.07s/it]\u001B[A\n",
      " 21%|██        | 52/248 [07:38<30:25,  9.31s/it]\u001B[A\n",
      " 21%|██▏       | 53/248 [07:47<29:12,  8.99s/it]\u001B[A\n",
      " 22%|██▏       | 54/248 [07:57<30:25,  9.41s/it]\u001B[A\n",
      " 22%|██▏       | 55/248 [08:06<29:53,  9.30s/it]\u001B[A\n",
      " 23%|██▎       | 56/248 [08:14<28:45,  8.99s/it]\u001B[A\n",
      " 23%|██▎       | 57/248 [08:24<29:14,  9.18s/it]\u001B[A\n",
      " 23%|██▎       | 58/248 [08:38<33:40, 10.63s/it]\u001B[A\n",
      " 24%|██▍       | 59/248 [08:46<30:56,  9.82s/it]\u001B[A\n",
      " 24%|██▍       | 60/248 [08:55<29:34,  9.44s/it]\u001B[A\n",
      " 25%|██▍       | 61/248 [09:04<29:17,  9.40s/it]\u001B[A\n",
      " 25%|██▌       | 62/248 [09:20<35:01, 11.30s/it]\u001B[A\n",
      " 25%|██▌       | 63/248 [09:27<31:14, 10.13s/it]\u001B[A\n",
      " 26%|██▌       | 64/248 [09:34<28:18,  9.23s/it]\u001B[A\n",
      " 26%|██▌       | 65/248 [09:44<28:37,  9.39s/it]\u001B[A\n",
      " 27%|██▋       | 66/248 [09:52<27:45,  9.15s/it]\u001B[A\n",
      " 27%|██▋       | 67/248 [10:01<27:00,  8.96s/it]\u001B[A\n",
      " 27%|██▋       | 68/248 [10:15<31:11, 10.40s/it]\u001B[A\n",
      " 28%|██▊       | 69/248 [10:23<29:21,  9.84s/it]\u001B[A\n",
      " 28%|██▊       | 70/248 [10:32<27:50,  9.39s/it]\u001B[A\n",
      " 29%|██▊       | 71/248 [10:48<34:10, 11.59s/it]\u001B[A\n",
      " 29%|██▉       | 72/248 [10:58<32:13, 10.98s/it]\u001B[A\n",
      " 29%|██▉       | 73/248 [11:06<29:25, 10.09s/it]\u001B[A\n",
      " 30%|██▉       | 74/248 [11:16<29:12, 10.07s/it]\u001B[A\n",
      " 30%|███       | 75/248 [11:24<27:21,  9.49s/it]\u001B[A\n",
      " 31%|███       | 76/248 [11:33<26:24,  9.21s/it]\u001B[A\n",
      " 31%|███       | 77/248 [11:41<25:34,  8.97s/it]\u001B[A\n",
      " 31%|███▏      | 78/248 [11:49<24:55,  8.80s/it]\u001B[A\n",
      " 32%|███▏      | 79/248 [11:57<24:00,  8.52s/it]\u001B[A\n",
      " 32%|███▏      | 80/248 [12:07<24:53,  8.89s/it]\u001B[A\n",
      " 33%|███▎      | 81/248 [12:15<24:15,  8.71s/it]\u001B[A\n",
      " 33%|███▎      | 82/248 [12:26<25:20,  9.16s/it]\u001B[A\n",
      " 33%|███▎      | 83/248 [12:32<23:21,  8.49s/it]\u001B[A\n",
      " 34%|███▍      | 84/248 [12:39<21:28,  7.86s/it]\u001B[A\n",
      " 34%|███▍      | 85/248 [12:49<23:19,  8.58s/it]\u001B[A\n",
      " 35%|███▍      | 86/248 [12:57<22:49,  8.46s/it]\u001B[A\n",
      " 35%|███▌      | 87/248 [13:05<21:55,  8.17s/it]\u001B[A\n",
      " 35%|███▌      | 88/248 [13:14<22:49,  8.56s/it]\u001B[A\n",
      " 36%|███▌      | 89/248 [13:21<21:34,  8.14s/it]\u001B[A\n",
      " 36%|███▋      | 90/248 [13:34<25:12,  9.57s/it]\u001B[A\n",
      " 37%|███▋      | 91/248 [13:42<23:35,  9.02s/it]\u001B[A\n",
      " 37%|███▋      | 92/248 [13:51<23:43,  9.13s/it]\u001B[A\n",
      " 38%|███▊      | 93/248 [13:58<21:55,  8.49s/it]\u001B[A\n",
      " 38%|███▊      | 94/248 [14:08<22:52,  8.91s/it]\u001B[A\n",
      " 38%|███▊      | 95/248 [14:25<28:17, 11.09s/it]\u001B[A\n",
      " 39%|███▊      | 96/248 [14:37<29:32, 11.66s/it]\u001B[A\n",
      " 39%|███▉      | 97/248 [14:48<28:39, 11.39s/it]\u001B[A\n",
      " 40%|███▉      | 98/248 [14:57<26:37, 10.65s/it]\u001B[A\n",
      " 40%|███▉      | 99/248 [15:04<23:40,  9.54s/it]\u001B[A\n",
      " 40%|████      | 100/248 [15:14<23:28,  9.52s/it]\u001B[A\n",
      " 41%|████      | 101/248 [15:22<22:17,  9.10s/it]\u001B[A\n",
      " 41%|████      | 102/248 [15:31<22:34,  9.28s/it]\u001B[A\n",
      " 42%|████▏     | 103/248 [15:39<21:12,  8.78s/it]\u001B[A\n",
      " 42%|████▏     | 104/248 [15:47<20:46,  8.66s/it]\u001B[A\n",
      " 42%|████▏     | 105/248 [15:58<21:40,  9.09s/it]\u001B[A\n",
      " 43%|████▎     | 106/248 [16:05<20:32,  8.68s/it]\u001B[A\n",
      " 43%|████▎     | 107/248 [16:14<20:09,  8.58s/it]\u001B[A\n",
      " 44%|████▎     | 108/248 [16:22<19:46,  8.48s/it]\u001B[A\n",
      " 44%|████▍     | 109/248 [16:32<21:00,  9.07s/it]\u001B[A\n",
      " 44%|████▍     | 110/248 [16:43<22:14,  9.67s/it]\u001B[A\n",
      " 45%|████▍     | 111/248 [16:51<20:31,  8.99s/it]\u001B[A\n",
      " 45%|████▌     | 112/248 [16:59<20:01,  8.83s/it]\u001B[A\n",
      " 46%|████▌     | 113/248 [17:08<19:31,  8.68s/it]\u001B[A\n",
      " 46%|████▌     | 114/248 [17:15<18:52,  8.45s/it]\u001B[A\n",
      " 46%|████▋     | 115/248 [17:24<18:37,  8.40s/it]\u001B[A\n",
      " 47%|████▋     | 116/248 [17:32<18:41,  8.50s/it]\u001B[A\n",
      " 47%|████▋     | 117/248 [17:40<17:43,  8.12s/it]\u001B[A\n",
      " 48%|████▊     | 118/248 [17:51<19:41,  9.09s/it]\u001B[A\n",
      " 48%|████▊     | 119/248 [17:58<18:28,  8.59s/it]\u001B[A\n",
      " 48%|████▊     | 120/248 [18:06<17:50,  8.36s/it]\u001B[A\n",
      " 49%|████▉     | 121/248 [18:14<17:08,  8.10s/it]\u001B[A\n",
      " 49%|████▉     | 122/248 [18:22<17:11,  8.18s/it]\u001B[A\n",
      " 50%|████▉     | 123/248 [18:32<17:49,  8.56s/it]\u001B[A\n",
      " 50%|█████     | 124/248 [18:40<17:28,  8.45s/it]\u001B[A\n",
      " 50%|█████     | 125/248 [18:44<14:46,  7.21s/it]\u001B[A\n",
      " 51%|█████     | 126/248 [18:50<13:46,  6.78s/it]\u001B[A\n",
      " 51%|█████     | 127/248 [18:54<11:55,  5.91s/it]\u001B[A\n",
      " 52%|█████▏    | 128/248 [18:58<10:56,  5.47s/it]\u001B[A\n",
      " 52%|█████▏    | 129/248 [19:03<10:20,  5.22s/it]\u001B[A\n",
      " 52%|█████▏    | 130/248 [19:06<09:12,  4.68s/it]\u001B[A\n",
      " 53%|█████▎    | 131/248 [19:10<08:47,  4.51s/it]\u001B[A\n",
      " 53%|█████▎    | 132/248 [19:14<08:11,  4.23s/it]\u001B[A\n",
      " 54%|█████▎    | 133/248 [19:20<08:54,  4.65s/it]\u001B[A\n",
      " 54%|█████▍    | 134/248 [19:23<08:04,  4.25s/it]\u001B[A\n",
      " 54%|█████▍    | 135/248 [19:27<07:54,  4.20s/it]\u001B[A\n",
      " 55%|█████▍    | 136/248 [19:34<09:31,  5.10s/it]\u001B[A\n",
      " 55%|█████▌    | 137/248 [19:38<08:38,  4.67s/it]\u001B[A\n",
      " 56%|█████▌    | 138/248 [19:42<08:08,  4.44s/it]\u001B[A\n",
      " 56%|█████▌    | 139/248 [19:46<07:47,  4.29s/it]\u001B[A\n",
      " 56%|█████▋    | 140/248 [19:50<07:32,  4.19s/it]\u001B[A\n",
      " 57%|█████▋    | 141/248 [19:54<07:34,  4.25s/it]\u001B[A\n",
      " 57%|█████▋    | 142/248 [19:59<07:44,  4.38s/it]\u001B[A\n",
      " 58%|█████▊    | 143/248 [20:02<07:20,  4.20s/it]\u001B[A\n",
      " 58%|█████▊    | 144/248 [20:07<07:22,  4.25s/it]\u001B[A\n",
      " 58%|█████▊    | 145/248 [20:11<07:10,  4.17s/it]\u001B[A\n",
      " 59%|█████▉    | 146/248 [20:17<07:51,  4.62s/it]\u001B[A\n",
      " 59%|█████▉    | 147/248 [20:21<07:34,  4.50s/it]\u001B[A\n",
      " 60%|█████▉    | 148/248 [20:24<06:50,  4.11s/it]\u001B[A\n",
      " 60%|██████    | 149/248 [20:29<07:12,  4.37s/it]\u001B[A\n",
      " 60%|██████    | 150/248 [20:34<07:25,  4.54s/it]\u001B[A\n",
      " 61%|██████    | 151/248 [20:37<06:51,  4.24s/it]\u001B[A\n",
      " 61%|██████▏   | 152/248 [20:41<06:33,  4.10s/it]\u001B[A\n",
      " 62%|██████▏   | 153/248 [20:46<06:57,  4.39s/it]\u001B[A\n",
      " 62%|██████▏   | 154/248 [20:51<07:13,  4.61s/it]\u001B[A\n",
      " 62%|██████▎   | 155/248 [20:56<07:02,  4.54s/it]\u001B[A\n",
      " 63%|██████▎   | 156/248 [21:00<06:54,  4.50s/it]\u001B[A\n",
      " 63%|██████▎   | 157/248 [21:03<06:17,  4.15s/it]\u001B[A\n",
      " 64%|██████▎   | 158/248 [21:07<06:07,  4.09s/it]\u001B[A\n",
      " 64%|██████▍   | 159/248 [21:11<06:00,  4.05s/it]\u001B[A\n",
      " 65%|██████▍   | 160/248 [21:14<05:18,  3.62s/it]\u001B[A\n",
      " 65%|██████▍   | 161/248 [21:18<05:27,  3.76s/it]\u001B[A\n",
      " 65%|██████▌   | 162/248 [21:23<05:52,  4.10s/it]\u001B[A\n",
      " 66%|██████▌   | 163/248 [21:29<06:33,  4.63s/it]\u001B[A\n",
      " 66%|██████▌   | 164/248 [21:32<05:59,  4.28s/it]\u001B[A\n",
      " 67%|██████▋   | 165/248 [21:37<05:55,  4.29s/it]\u001B[A\n",
      " 67%|██████▋   | 166/248 [21:42<06:15,  4.58s/it]\u001B[A\n",
      " 67%|██████▋   | 167/248 [21:46<05:52,  4.35s/it]\u001B[A\n",
      " 68%|██████▊   | 168/248 [21:50<05:58,  4.48s/it]\u001B[A\n",
      " 68%|██████▊   | 169/248 [21:54<05:37,  4.27s/it]\u001B[A\n",
      " 69%|██████▊   | 170/248 [21:58<05:23,  4.15s/it]\u001B[A\n",
      " 69%|██████▉   | 171/248 [22:02<05:09,  4.02s/it]\u001B[A\n",
      " 69%|██████▉   | 172/248 [22:05<04:45,  3.76s/it]\u001B[A\n",
      " 70%|██████▉   | 173/248 [22:08<04:30,  3.60s/it]\u001B[A\n",
      " 70%|███████   | 174/248 [22:12<04:35,  3.73s/it]\u001B[A\n",
      " 71%|███████   | 175/248 [22:16<04:33,  3.75s/it]\u001B[A\n",
      " 71%|███████   | 176/248 [22:19<04:20,  3.61s/it]\u001B[A\n",
      " 71%|███████▏  | 177/248 [22:23<04:09,  3.52s/it]\u001B[A\n",
      " 72%|███████▏  | 178/248 [22:30<05:33,  4.76s/it]\u001B[A\n",
      " 72%|███████▏  | 179/248 [22:34<05:02,  4.38s/it]\u001B[A\n",
      " 73%|███████▎  | 180/248 [22:38<04:51,  4.29s/it]\u001B[A\n",
      " 73%|███████▎  | 181/248 [22:42<04:49,  4.32s/it]\u001B[A\n",
      " 73%|███████▎  | 182/248 [22:45<04:20,  3.95s/it]\u001B[A\n",
      " 74%|███████▍  | 183/248 [22:50<04:22,  4.04s/it]\u001B[A\n",
      " 74%|███████▍  | 184/248 [22:53<04:14,  3.98s/it]\u001B[A\n",
      " 75%|███████▍  | 185/248 [22:58<04:17,  4.09s/it]\u001B[A\n",
      " 75%|███████▌  | 186/248 [23:01<04:02,  3.91s/it]\u001B[A\n",
      " 75%|███████▌  | 187/248 [23:05<04:01,  3.96s/it]\u001B[A\n",
      " 76%|███████▌  | 188/248 [23:10<04:11,  4.20s/it]\u001B[A\n",
      " 76%|███████▌  | 189/248 [23:15<04:24,  4.48s/it]\u001B[A\n",
      " 77%|███████▋  | 190/248 [23:19<04:10,  4.33s/it]\u001B[A\n",
      " 77%|███████▋  | 191/248 [23:23<03:57,  4.16s/it]\u001B[A\n",
      " 77%|███████▋  | 192/248 [23:27<03:50,  4.12s/it]\u001B[A\n",
      " 78%|███████▊  | 193/248 [23:32<03:57,  4.33s/it]\u001B[A\n",
      " 78%|███████▊  | 194/248 [23:37<04:07,  4.58s/it]\u001B[A\n",
      " 79%|███████▊  | 195/248 [23:40<03:44,  4.23s/it]\u001B[A\n",
      " 79%|███████▉  | 196/248 [23:44<03:36,  4.16s/it]\u001B[A\n",
      " 79%|███████▉  | 197/248 [23:48<03:24,  4.00s/it]\u001B[A\n",
      " 80%|███████▉  | 198/248 [23:52<03:16,  3.93s/it]\u001B[A\n",
      " 80%|████████  | 199/248 [23:56<03:19,  4.07s/it]\u001B[A\n",
      " 81%|████████  | 200/248 [24:00<03:09,  3.96s/it]\u001B[A\n",
      " 81%|████████  | 201/248 [24:04<03:06,  3.97s/it]\u001B[A\n",
      " 81%|████████▏ | 202/248 [24:08<03:02,  3.97s/it]\u001B[A\n",
      " 82%|████████▏ | 203/248 [24:12<02:59,  4.00s/it]\u001B[A\n",
      " 82%|████████▏ | 204/248 [24:16<02:52,  3.91s/it]\u001B[A\n",
      " 83%|████████▎ | 205/248 [24:20<05:06,  7.12s/it]\u001B[A\n",
      "Iteration:  79%|███████▉  | 499/632 [2:15:42<36:10, 16.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7728/744904450.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    145\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdistributed\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbarrier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 147\u001B[1;33m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msplited_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moriginal_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7728/1926992580.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(args, data, datasets, model, original_model, tokenizer)\u001B[0m\n\u001B[0;32m    538\u001B[0m                     \u001B[0mtrain_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0.0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    539\u001B[0m                     \u001B[1;31m# Log metrics\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 540\u001B[1;33m                     \u001B[0mbest_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbest_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msave_best_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbest_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbest_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_dataloaders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    541\u001B[0m                     \u001B[0mdev_dataloaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_example_num\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_distribution\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_dataloader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_datasets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    542\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7728/1926992580.py\u001B[0m in \u001B[0;36msave_best_model\u001B[1;34m(best_loss, best_step, dev_dataloaders)\u001B[0m\n\u001B[0;32m    192\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0msave_best_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbest_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbest_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_dataloaders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    193\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlocal_rank\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevaluate_during_training\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Only evaluate when single GPU otherwise metrics may not average well\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 194\u001B[1;33m             \u001B[0meval_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattributes_hiddens\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_dataloaders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    195\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\" global_step = %s, evaluate loss = %s\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meval_loss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m             \u001B[0mtb_writer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_scalar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"eval_loss\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meval_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7728/1926992580.py\u001B[0m in \u001B[0;36mevaluate\u001B[1;34m(model, attributes_hiddens, dev_dataloaders, prefix)\u001B[0m\n\u001B[0;32m    466\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdev_distribution\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    467\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 468\u001B[1;33m                 \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattributes_hiddens\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_dataloaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    469\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    470\u001B[0m                 \u001B[0meval_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7728/1926992580.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(attributes_hiddens, dataloaders, key)\u001B[0m\n\u001B[0;32m    307\u001B[0m                     \u001B[0mtoken_original\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moriginal_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlm_head\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinal_layer_original_hiddens\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    308\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_type\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'bert'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 309\u001B[1;33m             \u001B[0mall_layer_hiddens\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    310\u001B[0m             \u001B[0mfinal_layer_hiddens\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlast_hidden_state\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    311\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;34m'neutral'\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    994\u001B[0m             \u001B[0mpast_key_values_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpast_key_values_length\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    995\u001B[0m         )\n\u001B[1;32m--> 996\u001B[1;33m         encoder_outputs = self.encoder(\n\u001B[0m\u001B[0;32m    997\u001B[0m             \u001B[0membedding_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    998\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mextended_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    583\u001B[0m                 )\n\u001B[0;32m    584\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 585\u001B[1;33m                 layer_outputs = layer_module(\n\u001B[0m\u001B[0;32m    586\u001B[0m                     \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    587\u001B[0m                     \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    511\u001B[0m             \u001B[0mpresent_key_value\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpresent_key_value\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mcross_attn_present_key_value\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    512\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 513\u001B[1;33m         layer_output = apply_chunking_to_forward(\n\u001B[0m\u001B[0;32m    514\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeed_forward_chunk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchunk_size_feed_forward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseq_len_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    515\u001B[0m         )\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\modeling_utils.py\u001B[0m in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m   2436\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_chunks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mchunk_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2437\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2438\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mforward_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput_tensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2439\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2440\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mfeed_forward_chunk\u001B[1;34m(self, attention_output)\u001B[0m\n\u001B[0;32m    523\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    524\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfeed_forward_chunk\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 525\u001B[1;33m         \u001B[0mintermediate_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    526\u001B[0m         \u001B[0mlayer_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mintermediate_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    527\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlayer_output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    425\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    426\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 427\u001B[1;33m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate_act_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    428\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    429\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mgelu\u001B[1;34m(input)\u001B[0m\n\u001B[0;32m   1554\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhas_torch_function_unary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1555\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgelu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1556\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1557\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1558\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if (\n",
    "    os.path.exists(args.output_dir)\n",
    "    and os.listdir(args.output_dir)\n",
    "    and args.do_train\n",
    "    and not args.overwrite_output_dir\n",
    "    and not args.should_continue\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Setup distant debugging if needed\n",
    "if args.server_ip and args.server_port:\n",
    "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "    import ptvsd\n",
    "\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "set_seed(args)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    args.local_rank,\n",
    "    device,\n",
    "    args.n_gpu,\n",
    "    bool(args.local_rank != -1),\n",
    "    args.fp16,\n",
    ")\n",
    "\n",
    " # Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
    "else:\n",
    "    # When we release a pip version exposing CONFIG_MAPPING,\n",
    "    # we can do `config = CONFIG_MAPPING[args.model_type]()`.\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new config instance from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "        \"and load it from here, using --config_name\"\n",
    "    )\n",
    "\n",
    "config.output_hidden_states = 'true'\n",
    "\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "        \"and load it from here, using --tokenizer_name\"\n",
    "    )\n",
    "\n",
    "if args.block_size <= 0:\n",
    "    args.block_size = tokenizer.model_max_length\n",
    "    # Our input block size will be the max possible for the model\n",
    "else:\n",
    "    args.block_size = min(args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "if args.model_name_or_path:\n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    original_model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelWithLMHead.from_config(config)\n",
    "    original_model = AutoModelWithLMHead.from_config(config)\n",
    "\n",
    "# GPT-2 and GPT do not have pad.\n",
    "if tokenizer._pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    original_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(args.device)\n",
    "original_model.to(args.device)\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "attributes_examples=[]\n",
    "attributes_labels=[]\n",
    "neutral_examples=[]\n",
    "neutral_exist=0\n",
    "for data_file in args.data_files:\n",
    "    data=torch.load(data_file)\n",
    "    attributes_examples.append(data['attributes_examples'])\n",
    "    attributes_labels.append(data['attributes_labels'])\n",
    "    neutral_examples.append(data['neutral_examples'])\n",
    "\n",
    "    if 'neutral_labels' in data:\n",
    "        neutral_exist+=1\n",
    "\n",
    "if neutral_exist==len(args.data_files):\n",
    "    neutral_labels=[]\n",
    "    for data_file in args.data_files:\n",
    "        data=torch.load(data_file)\n",
    "        neutral_labels.append(data['neutral_labels'])\n",
    "    splited_data=split_data(attributes_examples, attributes_labels, neutral_examples, neutral_labels, args)\n",
    "else:\n",
    "    splited_data=split_data(attributes_examples, attributes_labels, neutral_examples, None, args)\n",
    "\n",
    "datasets = load_and_cache_examples(splited_data, args, tokenizer)\n",
    "\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "train(args, splited_data, datasets, model, original_model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}