{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gzip\n",
    "import regex as re\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import *\n",
    "import os.path\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "this_model_type='albert' #from ['bert', 'roberta', 'albert', 'dbert', 'electra', 'gpt2']\n",
    "this_block_size=128\n",
    "attributes=['./attribute_target_words/attributes/religion_demonyms.txt']\n",
    "stereotypes_file='./attribute_target_words/targets/polarized_and_class_words.txt'  #not using stereotypes; define ''\n",
    "out_combo='religion_polarized_class'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "if not os.path.exists('news_commentary_v15.en'):\n",
    "    f_in=gzip.open('news-commentary-v15.en.gz') #download from website; or use curl\n",
    "    f_out=open('news_commentary_v15.en', 'wb')\n",
    "    f_out.writelines(f_in)\n",
    "    f_out.close()\n",
    "    f_in.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "with open('news_commentary_v15.en', 'r', encoding='utf-8') as f:\n",
    "    lines=f.readlines()\n",
    "\n",
    "data=[l.strip() for l in lines]\n",
    "\n",
    "if stereotypes_file:\n",
    "    stereotypes=[word.strip() for word in open(stereotypes_file)]\n",
    "    stereotype_set=set(stereotypes)\n",
    "\n",
    "pat=re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "attributes_l=[]\n",
    "all_attributes_set=set()\n",
    "for attribute in attributes:\n",
    "    l=[word.strip() for word in open(attribute)]\n",
    "    attributes_l.append(set(l))\n",
    "    all_attributes_set |= set(l)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def prepare_transformer(model_type):\n",
    "    if model_type=='bert':\n",
    "        pretrained_weights='bert-base-uncased'\n",
    "        model=BertModel.from_pretrained(pretrained_weights, output_hidden_states=True)\n",
    "        tokenizer=BertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='roberta':\n",
    "        pretrained_weights='roberta-base'\n",
    "        model=RobertaModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=RobertaTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='albert':\n",
    "        pretrained_weights='albert-base-v2'\n",
    "        model=AlbertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=AlbertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='dbert':\n",
    "        pretrained_weights='distilbert-base-uncased'\n",
    "        model=DistilBertModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xlnet':\n",
    "        pretrained_weights='xlnet-base-cased'\n",
    "        model=XLNetModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=XLNetTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='electra':\n",
    "        pretrained_weights='google/electra-small-discriminator'\n",
    "        model=ElectraModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=ElectraTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt':\n",
    "        pretrained_weights='openai-gpt'\n",
    "        model=OpenAIGPTModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=OpenAIGPTTokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='gpt2':\n",
    "        pretrained_weights='gpt2'\n",
    "        model=GPT2Model.from_pretrained(pretrained_weights)\n",
    "        tokenizer=GPT2Tokenizer.from_pretrained(pretrained_weights)\n",
    "    elif model_type=='xl':\n",
    "        pretrained_weights='transfo-xl-wt103'\n",
    "        model=TransfoXLModel.from_pretrained(pretrained_weights)\n",
    "        tokenizer=TransfoXLTokenizer.from_pretrained(pretrained_weights)\n",
    "    return model, tokenizer\n",
    "\n",
    "def encode_to_is(tokenizer, the_data, add_special_tokens):\n",
    "    if type(the_data)==list:\n",
    "        data=[tuple(tokenizer.encode(sentence, add_special_tokens=add_special_tokens)) for sentence in the_data]\n",
    "    elif type(the_data)==dict:\n",
    "        data={tuple(tokenizer.encode(key, add_special_tokens=add_special_tokens)): tokenizer.encode(value, add_special_tokens=add_special_tokens) for key, value in the_data.items()}\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of AlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n",
      "loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at C:\\Users\\31631/.cache\\huggingface\\transformers\\e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer=prepare_transformer(this_model_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "if stereotypes_file:\n",
    "    tok_stereotypes=encode_to_is(tokenizer, stereotypes, add_special_tokens=False)\n",
    "\n",
    "neutral_examples=[]\n",
    "if stereotypes_file:\n",
    "    neutral_labels=[]\n",
    "attribute_examples=[[] for _ in range(len(attributes_l))]\n",
    "attribute_labels=[[] for _ in range(len(attributes_l))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608912/608912 [01:24<00:00, 7209.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral: 34439\n",
      "attributes0: 6443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for line in tqdm(data):\n",
    "    neutral_flag=True\n",
    "    line=line.strip()\n",
    "    if len(line)<1:\n",
    "        continue\n",
    "    length=len(line.split())\n",
    "    if length>this_block_size or length<=1:\n",
    "        continue\n",
    "    tokens_orig=[token.strip() for token in re.findall(pat, line)]\n",
    "    tokens_lower=[token.lower() for token in tokens_orig]\n",
    "    token_set=set(tokens_lower)\n",
    "\n",
    "    attribute_other_l=[]\n",
    "    for i, _ in enumerate(attributes_l):\n",
    "        a_set=set()\n",
    "        for j, attribute in enumerate(attributes_l):\n",
    "            if i!=j:\n",
    "                a_set |= attribute\n",
    "        attribute_other_l.append(a_set)\n",
    "\n",
    "    for i, (attribute_set, other_set) in enumerate(zip(attributes_l, attribute_other_l)):\n",
    "        # & is bitwise AND operator\n",
    "        if attribute_set & token_set: #if a gender attribute is in the data line; classify the line as not neutral; and set the attribute to be the label\n",
    "            neutral_flag=False\n",
    "            if not other_set&token_set:\n",
    "                orig_line=line\n",
    "                line=tokenizer.encode(line, add_special_tokens=True)\n",
    "                labels=attribute_set & token_set\n",
    "                for label in list(labels):\n",
    "                    idx=tokens_lower.index(label)\n",
    "                label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "                line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                if label not in line_ngram:\n",
    "                    label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                    line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram:\n",
    "                        label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                        line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                        if label not in line_ngram:\n",
    "                            label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                            line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                idx=line_ngram.index(label)\n",
    "                attribute_examples[i].append(line)\n",
    "                attribute_labels[i].append([idx+j for j in range(len(label))])\n",
    "            break\n",
    "\n",
    "    if neutral_flag:\n",
    "        if stereotypes_file:\n",
    "            if stereotype_set & token_set: #if attribute is not in line; check if stereotype in line: if there is a stereotype in the line, but not an attribute: the line is considered neutral\n",
    "                orig_line=line\n",
    "                line=tokenizer.encode(line, add_special_tokens=True)\n",
    "                labels=stereotype_set&token_set #stereotype words present in line are the labels\n",
    "                for label in list(labels):\n",
    "                    idx=tokens_lower.index(label)\n",
    "                    label=tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=True))[1:-1]\n",
    "                    line_ngram=list(nltk.ngrams(line, len(label)))\n",
    "                    if label not in line_ngram:\n",
    "                        label = tuple(tokenizer.encode(tokens_orig[idx], add_special_tokens=False))\n",
    "                        line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                        if label not in line_ngram:\n",
    "                            label = tuple(tokenizer.encode(f'a {tokens_orig[idx]} a'))[1:-1]\n",
    "                            line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                            if label not in line_ngram:\n",
    "                                label = tuple([tokenizer.encode(f'{tokens_orig[idx]}2')[0]])\n",
    "                                line_ngram = list(nltk.ngrams(line, len(label)))\n",
    "                    idx=line_ngram.index(label)\n",
    "                    neutral_examples.append(line)\n",
    "                    neutral_labels.append([idx+i for i in range(len(label))])\n",
    "        else:\n",
    "            neutral_examples.append(tokenizer.encode(line, add_special_tokens=True)) #If we dont use stereotypes; all lines not containing an attribute word are considered neutral\n",
    "\n",
    "print('neutral:', len(neutral_examples))\n",
    "for i, examples in enumerate(attribute_examples):\n",
    "    print(f'attributes{i}:', len(examples))\n",
    "\n",
    "data_output= {'attributes_examples': attribute_examples,\n",
    "            'attributes_labels': attribute_labels,\n",
    "            'neutral_examples': neutral_examples}\n",
    "\n",
    "if stereotypes_file:\n",
    "    data_output['neutral_labels']=neutral_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "torch.save(data_output, './data/'+this_model_type+'/'+out_combo+'_data.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}