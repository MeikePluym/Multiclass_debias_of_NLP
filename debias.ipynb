{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import copy\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (MODEL_WITH_LM_HEAD_MAPPING, WEIGHTS_NAME, AdamW, AutoConfig, AutoModelWithLMHead, AutoTokenizer, PreTrainedModel, PreTrainedTokenizer, get_linear_schedule_with_warmup)\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class Args:\n",
    "    #model architecture to be trained/fine-tuned\n",
    "    model_type='bert'   #['bert', 'roberta', 'albert', 'dbert', 'electra', 'gpt2']\n",
    "    #weights for loss function\n",
    "    alpha=0.2\n",
    "    beta=1-alpha\n",
    "    #number of attributes in attribute dict; always 1 (?)\n",
    "    num_attributes=1\n",
    "    #keep training from checkpoint; train next epoch (fine-tuning). False if from scratch\n",
    "    checkpoint=False\n",
    "\n",
    "    #input data file\n",
    "    #data_files=['./data/'+model_type+'/gender_stereotype_data.bin', './data/'+model_type+'/ethnicities_polarized_class_data.bin']\n",
    "    data_file='./data/'+model_type+'/gender_stereotype_data.bin'\n",
    "    #data_file_ethnicities='./data/'+model_type+'/ethnicities_polarized_class_data.bin'\n",
    "\n",
    "    #output directory where model predictions/checkpoints will be written\n",
    "    output_dir='./debiased_models/'+model_type\n",
    "\n",
    "    #model checkpoint for weights initialization; None if you want to train model from scratch\n",
    "    if model_type=='bert':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/bert/checkpoint-best/'\n",
    "        else:\n",
    "            model_name_or_path='bert-base-uncased'\n",
    "    elif model_type=='roberta':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/roberta/checkpoint-best/'\n",
    "        else:\n",
    "            model_name_or_path='roberta-base'\n",
    "    elif model_type=='albert':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/albert/checkpoint-best/'\n",
    "        else:\n",
    "            model_name_or_path='albert-base-v2'\n",
    "    elif model_type=='dbert':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/dbert/checkpoint-best'\n",
    "        else:\n",
    "            model_name_or_path='distilbert-base-uncased'\n",
    "    elif model_type=='electra':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/electra/checkpoint-best'\n",
    "        else:\n",
    "            model_name_or_path='google/electra-small-discriminator'\n",
    "    elif model_type=='gpt2':\n",
    "        if checkpoint:\n",
    "            model_name_or_path='./debiased_models/gpt2/checkpoint-best'\n",
    "        else:\n",
    "            model_name_or_path='gpt2'\n",
    "\n",
    "    #optional pretrained config name/path if not same as model_name_or_path\n",
    "    config_name=''\n",
    "\n",
    "    #optional pretrained tokenizer name/path\n",
    "    tokenizer_name=''\n",
    "\n",
    "    #optional input evaluation data file\n",
    "    eval_data_file=None\n",
    "\n",
    "    #whether distinct lines of text in data are to be handled as distinct sequences\n",
    "    line_by_line=True\n",
    "\n",
    "    #whether to continue from latest checkpoint in output_dir\n",
    "    should_continue=False\n",
    "\n",
    "    #loss target; sentence or token\n",
    "    loss_target=\"sentence\"\n",
    "\n",
    "    #train with MLM loss instead of language modeling\n",
    "    mlm=False\n",
    "\n",
    "    #ratio of tokens to mask for MLM loss\n",
    "    mlm_probability=0.15\n",
    "\n",
    "    #optional directory to store pre-trained models\n",
    "    cache_dir=None\n",
    "\n",
    "    #optional input sequence length after tokenization; training data will be truncated. Default; max input length for single sentence inputs\n",
    "    block_size=128\n",
    "\n",
    "    #whether to run training\n",
    "    do_train=True\n",
    "    #whether to run eval\n",
    "    do_eval=True\n",
    "    #run eval during training at each logging step\n",
    "    evaluate_during_training=True\n",
    "\n",
    "    #batch size for train\n",
    "    per_gpu_train_batch_size=32\n",
    "    #bathc size for eval\n",
    "    per_gpu_eval_batch_size=32\n",
    "\n",
    "    #numb. of update steps to accumulate before performing backward/update pass\n",
    "    gradient_accumulation_steps=1\n",
    "    #initial learning rate for Adam\n",
    "    learning_rate=5e-5\n",
    "    #weight decay (if applied)\n",
    "    weight_decay=0.0\n",
    "    #epsilon for Adam optimizer\n",
    "    adam_epsilon= 1e-8\n",
    "    #max gradient norm.\n",
    "    max_grad_norm=1.0\n",
    "    #weighted loss\n",
    "    weighted_loss=[alpha, beta]\n",
    "    #['all', 'first', 'last'] to debias\n",
    "    debias_layer='all'\n",
    "    #number of training epochs\n",
    "    num_train_epochs=2\n",
    "    #if >0; set total number of training steps to perform\n",
    "    max_steps=-1\n",
    "    #linear warmup\n",
    "    warmup_steps=0\n",
    "    #square loss\n",
    "    square_loss=True\n",
    "    #token loss\n",
    "    token_loss=False\n",
    "    #log every x update steps\n",
    "    logging_steps=500\n",
    "    #save checkpoint every x update steps\n",
    "    save_steps=500\n",
    "    #limit total amount of checkpoints, delete older in output_dir\n",
    "    save_total_limit=None\n",
    "    #evaluate all checkpoints\n",
    "    eval_all_checkpoints=False\n",
    "    #avoid using CUDA when available\n",
    "    no_cuda=True\n",
    "    #overwrite content of output directory\n",
    "    overwrite_output_dir=True\n",
    "    #overwrite cached training and eval sets\n",
    "    overwrite_cache=False\n",
    "    #random seed for initialization\n",
    "    seed=42\n",
    "    #evaluation data set size\n",
    "    dev_data_size=100\n",
    "    train_data_size=1000 #LET OP HIER DATA SET KLEINER GEMAAKT\n",
    "\n",
    "    #whether to use 16-bit (mixed) precision instead of 32-bit\n",
    "    fp16=False\n",
    "    #Apex AMP opt level selected ['00', '01', '02', '03']\n",
    "    fp16_opt_level=\"01\"\n",
    "\n",
    "    tp=lambda x:list(x.split(','))\n",
    "    #exclusion list\n",
    "    exclusion_list=[]\n",
    "    #for distributed training\n",
    "    local_rank=-1\n",
    "    #for distant debugging\n",
    "    server_ip=\"\"\n",
    "    #for distant debugging\n",
    "    server_port=\"\"\n",
    "\n",
    "    n_gpu=0\n",
    "\n",
    "args=Args()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "logger=logging.getLogger(__name__)\n",
    "MODEL_CONFIG_CLASSES=list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES=tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
    "        assert os.path.isfile(file_path)\n",
    "\n",
    "        block_size=block_size-(tokenizer.model_max_length - tokenizer.max_model_input_sizes)\n",
    "\n",
    "        directory, filename=os.path.split(file_path)\n",
    "        cached_features_file=os.path.join(directory, args.model_type + \"_cached_lm_\"+str(block_size)+\"_\"+filename)\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples=pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "            self.examples=[]\n",
    "            with open(file_path, encoding=\"utf-8\") as f:\n",
    "                text=f.read()\n",
    "\n",
    "            tokenized_text=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "            for i in range(0, len(tokenized_text)-block_size+1, block_size):\n",
    "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dumpt(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, examples: list, labels: list):\n",
    "        self.examples=examples\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.labels:\n",
    "            return torch.tensor(self.examples[i], dtype=torch.long), torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        else:\n",
    "            return torch.tensor(self.examples[i], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def create_dataset(data, dataset):\n",
    "    d=dict()\n",
    "    for key in data['example'].keys():\n",
    "        if key not in data['label']:\n",
    "            d[key]=dataset(data['example'][key], None)\n",
    "        else:\n",
    "            d[key]=dataset(data['example'][key], data['label'][key])\n",
    "    return d\n",
    "\n",
    "def load_and_cache_examples(data, args, tokenizer):\n",
    "    if args.line_by_line:\n",
    "        train_dataset=create_dataset(data['train'], LineByLineTextDataset)\n",
    "        dev_dataset=create_dataset(data['dev'], LineByLineTextDataset)\n",
    "        return {'train': train_dataset, 'dev': dev_dataset}\n",
    "    #else:\n",
    "     #   return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
    "    #NB; WHAT IS FILE PATH HERE? WILL GIVE ERROR OF LINE_BY_LINE=FALSE\n",
    "\n",
    "def split_data(attributes_examples, attributes_labels, neutral_examples, neutral_labels, args):\n",
    "    data_full = {'train': {'example': {}, 'label': {}}, 'dev': {'example': {}, 'label': {}}}\n",
    "    data = {'train': {'example': {}, 'label': {}}, 'dev': {'example': {}, 'label': {}}}\n",
    "\n",
    "    for i, (examples, labels) in enumerate(zip(attributes_examples, attributes_labels)):\n",
    "        idx_l=list(range(len(examples)))\n",
    "        examples=[examples[idx] for idx in idx_l]\n",
    "        labels=[labels[idx] for idx in idx_l]\n",
    "        data['train']['example'][f'attribute{i}'] = examples[args.dev_data_size:args.train_data_size]\n",
    "        data['train']['label'][f'attribute{i}'] = labels[args.dev_data_size:args.train_data_size]\n",
    "        data['dev']['example'][f'attribute{i}'] = examples[:args.dev_data_size]\n",
    "        data['dev']['label'][f'attribute{i}'] = labels[:args.dev_data_size]\n",
    "\n",
    "    idx_l=list(range(len(neutral_examples)))\n",
    "    random.shuffle(idx_l)\n",
    "    neutral_examples = [neutral_examples[idx] for idx in idx_l]\n",
    "    data['train']['example']['neutral'] = neutral_examples[args.dev_data_size:args.train_data_size]\n",
    "    data['dev']['example']['neutral'] = neutral_examples[:args.dev_data_size]\n",
    "    if neutral_labels is not None:\n",
    "        neutral_labels = [neutral_labels[idx] for idx in idx_l]\n",
    "        data['train']['label']['neutral'] = neutral_labels[args.dev_data_size:args.train_data_size]\n",
    "        data['dev']['label']['neutral'] = neutral_labels[:args.dev_data_size]\n",
    "    return data\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "def create_dataloader(args, datasets, tokenizer, train=False):\n",
    "    def collate(batch: List[torch.Tensor]):\n",
    "        if type(batch[0])==tuple:\n",
    "            examples, labels=list(zip(*batch))\n",
    "            return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id), pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "        else:\n",
    "            return pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    dataloaders={}\n",
    "    example_num=0\n",
    "    data_distribution=[]\n",
    "\n",
    "    max_size = max([len(value) for key, value in datasets.items() if key != 'neutral'])\n",
    "    min_size = min([len(value) for key, value in datasets.items() if key != 'neutral'])\n",
    "\n",
    "    for key, dataset in datasets.items():\n",
    "        example_num+=len(dataset)\n",
    "        if train:\n",
    "            #CHECK PER GPU BATCH SIZE VS TRAIN BATCH SIZE\n",
    "            dataloaders[key]=iter(DataLoader(dataset, batch_size=args.per_gpu_train_batch_size, collate_fn=collate, shuffle=True))\n",
    "            data_distribution+=[key for _ in range(int(min_size/args.per_gpu_train_batch_size))]\n",
    "        else:\n",
    "            dataloaders[key]=iter(DataLoader(dataset, batch_size=args.per_gpu_eval_batch_size, collate_fn=collate, shuffle=True))\n",
    "            data_distribution+=[key for _ in range(int(min_size/args.per_gpu_eval_batch_size))]\n",
    "    return dataloaders, example_num, data_distribution\n",
    "\n",
    "def train(args, data,  datasets, model: PreTrainedModel, original_model, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer=SummaryWriter()\n",
    "\n",
    "    args.train_batch_size=args.per_gpu_train_batch_size*max(1, args.n_gpu)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    train_datasets=datasets['train']\n",
    "    dev_datasets=datasets['dev']\n",
    "\n",
    "    train_dataloaders, train_example_num, train_distribution=create_dataloader(args, train_datasets, tokenizer, train=True)\n",
    "    dev_dataloaders, dev_example_num, dev_distribution = create_dataloader(args, dev_datasets, tokenizer, train=False)\n",
    "\n",
    "    train_iter_num=sum([len(dataloader) for dataloader in train_dataloaders.values()])\n",
    "    dev_iter_num=sum([len(dataloader) for dataloader in dev_dataloaders.values()])\n",
    "\n",
    "    if args.max_steps>0:\n",
    "        t_total=args.max_steps\n",
    "        args.num_train_epochs=args.max_steps//(train_iter_num//args.gradient_accumulation_steps)+1\n",
    "    else:\n",
    "        t_total = train_iter_num // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model=model.module if hasattr(model, \"module\") else model\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    original_model = original_model.module if hasattr(original_model, \"module\") else original_model  # Take care of distributed/parallel training\n",
    "    original_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    #Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters=[\n",
    "        {\n",
    "            \"params\": [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        original_model = torch.nn.DataParallel(original_model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n",
    "        original_model = torch.nn.parallel.DistributedDataParallel(\n",
    "            original_model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n",
    "\n",
    "    #Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", train_example_num)\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step=0\n",
    "    epochs_trained=0\n",
    "    best_loss=float('inf')\n",
    "    best_step=0\n",
    "    steps_trained_in_current_epoch=0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (train_iter_num // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (train_iter_num // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    model.zero_grad()\n",
    "    original_model.zero_grad()\n",
    "    #train_iterator=trange(epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    train_iterator=range(epochs_trained, int(args.num_train_epochs))\n",
    "\n",
    "    def inner_product(x,y):\n",
    "        return torch.mean(torch.sum(y*x, 3))\n",
    "\n",
    "    def mean_square(x,y,idx):\n",
    "        return torch.mean(torch.mean((y-x)**2, idx))\n",
    "\n",
    "    def save_best_model(best_loss, best_step, dev_dataloaders):\n",
    "        if (args.local_rank == -1 and args.evaluate_during_training):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "            eval_loss = evaluate(model, attributes_hiddens, dev_dataloaders)\n",
    "            logger.info(\" global_step = %s, evaluate loss = %s\", global_step, eval_loss)\n",
    "            tb_writer.add_scalar(\"eval_loss\", eval_loss, global_step)\n",
    "        tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "\n",
    "        if eval_loss<best_loss:\n",
    "            best_loss = eval_loss\n",
    "            best_step = global_step\n",
    "            checkpoint_prefix = \"checkpoint\"\n",
    "            # Save model checkpoint\n",
    "            output_dir = os.path.join(args.output_dir, \"debiased-checkpoint-best\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            model_to_save = (\n",
    "                model.module if hasattr(model, \"module\") else model\n",
    "            )  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "        logger.info(\" best_step = %s, best loss = %s\", best_step, best_loss)\n",
    "\n",
    "        original_output_dir=os.path.join(args.output_dir, \"original-checkpoint-best\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        original_model_to_save=(\n",
    "            original_model.module if hasattr(original_model, \"module\") else original_model\n",
    "        )\n",
    "        original_model_to_save.save_pretrained(original_output_dir)\n",
    "        tokenizer.save_pretrained(original_output_dir)\n",
    "\n",
    "        torch.save(args, os.path.join(original_output_dir, \"training_args.bin\"))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(original_output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(original_output_dir, \"scheduler.pt\"))\n",
    "        return best_loss, best_step\n",
    "\n",
    "    def get_hiddens_of_model(input):\n",
    "        model.zero_grad()\n",
    "        if args.model_type == 'roberta':\n",
    "            #_, _, hiddens = model.roberta(input)\n",
    "            hiddens=model.roberta(input).hidden_states\n",
    "        elif args.model_type == 'bert':\n",
    "            #changed\n",
    "            hiddens=model.bert(input).hidden_states\n",
    "        elif args.model_type == 'albert':\n",
    "            #_, _, hiddens = model.albert(input)\n",
    "            hiddens=model.albert(input).hidden_states\n",
    "        elif args.model_type == 'dbert':\n",
    "            #changed\n",
    "            #_, hiddens = model.distilbert(input)\n",
    "            hiddens=model.distilbert(input).hidden_states\n",
    "        elif args.model_type == 'electra':\n",
    "            _, hiddens = model.electra(input)\n",
    "        elif args.model_type == 'gpt2':\n",
    "            _, _, hiddens = model.transformer(input)\n",
    "        elif args.model_type == 'gpt':\n",
    "            _, hiddens = model.transformer(input)\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "    def attribute_vector_example():\n",
    "        attributes_hiddens = {f'attribute{i}': [] for i in range(args.num_attributes)}\n",
    "\n",
    "        dataloaders, _, distribution = create_dataloader(args, train_datasets, tokenizer, train=True)\n",
    "        for key in distribution:\n",
    "            if key != 'neutral':\n",
    "                inputs, labels = next(dataloaders[key])\n",
    "                inputs = inputs.to(args.device)\n",
    "                hiddens = get_hiddens_of_model(inputs)\n",
    "                hiddens = torch.stack(hiddens, 2)\n",
    "                if labels.size(1) > 1:\n",
    "                    onehot = torch.eye(hiddens.size(1))\n",
    "                    zeros = torch.zeros(1, onehot.size(0))\n",
    "                    onehot = torch.cat((zeros, onehot), 0)\n",
    "                    onehot = onehot[labels]\n",
    "                    onehot = torch.sum(onehot, 1)\n",
    "                    onehot = onehot.view(hiddens.size(0), -1, 1, 1)\n",
    "                else:\n",
    "                    onehot = torch.eye(hiddens.size(1))[labels].view(hiddens.size(0), -1, 1, 1)\n",
    "                onehot = onehot.to(args.device)\n",
    "                attributes_hiddens[key].append(torch.sum(hiddens * onehot, 1) / labels.size(1))\n",
    "\n",
    "        # neutral\n",
    "        attribute_size = len(data['train']['example'])\n",
    "        for i in range(attribute_size - 1):\n",
    "            attributes_hiddens[f'attribute{i}'] = torch.mean(torch.cat(attributes_hiddens[f'attribute{i}'], 0), 0).detach().unsqueeze(0)\n",
    "\n",
    "        return attributes_hiddens\n",
    "\n",
    "    def forward(attributes_hiddens, dataloaders, key):\n",
    "        inputs = next(dataloaders[key])\n",
    "        if len(inputs) == 2:\n",
    "            inputs, labels = inputs\n",
    "            labels = labels.to(args.device)\n",
    "        else:\n",
    "            labels = None\n",
    "        inputs = inputs.to(args.device)\n",
    "        if args.model_type == 'roberta':\n",
    "            #final_layer_hiddens, first_token_hidden, all_layer_hiddens = model.roberta(inputs)\n",
    "            all_layer_hiddens=model.roberta(inputs).hidden_states\n",
    "            final_layer_hiddens=model.roberta(inputs).last_hidden_state\n",
    "            if 'neutral' != key:\n",
    "                with torch.no_grad():\n",
    "                    #final_layer_original_hiddens, _, all_layer_original_hiddens = original_model.roberta(inputs)\n",
    "                    all_layer_original_hiddens=original_model.roberta(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.roberta(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.lm_head(final_layer_hiddens)\n",
    "                    token_original = original_model.lm_head(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'bert':\n",
    "            all_layer_hiddens=model.bert(inputs).hidden_states\n",
    "            final_layer_hiddens=model.bert(inputs).last_hidden_state\n",
    "            if 'neutral' != key:\n",
    "                with torch.no_grad():\n",
    "                    all_layer_original_hiddens=original_model.bert(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.bert(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.cls(final_layer_hiddens)\n",
    "                    token_original = original_model.cls(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'albert':\n",
    "            #final_layer_hiddens, first_token_hidden, all_layer_hiddens = model.albert(inputs)\n",
    "            all_layer_hiddens=model.albert(inputs).hidden_states\n",
    "            final_layer_hiddens=model.albert(inputs).last_hidden_state\n",
    "            if 'neutral' != key:\n",
    "                with torch.no_grad():\n",
    "                    #final_layer_original_hiddens, _, all_layer_original_hiddens = original_model.albert(inputs)\n",
    "                    all_layer_original_hiddens=original_model.albert(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.albert(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.classifier(final_layer_hiddens)\n",
    "                    token_original = original_model.classifier(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'dbert':\n",
    "            #final_layer_hiddens, all_layer_hiddens = model.distilbert(inputs)\n",
    "            all_layer_hiddens=model.distilbert(inputs).hidden_states\n",
    "            final_layer_hiddens=model.distilbert(inputs).last_hidden_state\n",
    "            if 'neutral' != key:\n",
    "                with torch.no_grad():\n",
    "                    #final_layer_original_hiddens, all_layer_original_hiddens = original_model.distilbert(inputs)\n",
    "                    all_layer_original_hiddens=original_model.distilbert(inputs).hidden_states\n",
    "                    final_layer_original_hiddens=original_model.distilbert(inputs).last_hidden_state\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.classifier(final_layer_hiddens)\n",
    "                    token_original = original_model.classifier(final_layer_original_hiddens)\n",
    "        elif args.model_type == 'electra':\n",
    "            final_layer_hiddens, all_layer_hiddens = model.electra(inputs)\n",
    "            if 'neutral' != key:\n",
    "                with torch.no_grad():\n",
    "                    final_layer_original_hiddens, all_layer_original_hiddens = original_model.electra(inputs)\n",
    "                if args.token_loss:\n",
    "                    hiddens = model.generator_predictions(final_layer_hiddens)\n",
    "                    token_predicts = model.generator_lm_head(hiddens)\n",
    "                    original_hiddens = original_model.generator_predictions(final_layer_original_hiddens)\n",
    "                    token_original = original_model.generator_lm_head(original_hiddens)\n",
    "        elif args.model_type == 'gpt2':\n",
    "            final_layer_hiddens, first_token_hidden, all_layer_hiddens = model.transformer(inputs)\n",
    "            if 'neutral' != key:\n",
    "                with torch.no_grad():\n",
    "                   final_layer_original_hiddens, _, all_layer_original_hiddens = original_model.transformer(inputs)\n",
    "                if args.token_loss:\n",
    "                    token_predicts = model.lm_head(final_layer_hiddens)\n",
    "                    token_original = original_model.lm_head(final_layer_original_hiddens)\n",
    "\n",
    "        all_layer_hiddens = torch.stack(all_layer_hiddens, 2)\n",
    "        if 'neutral' != key:\n",
    "            all_original_hiddens =  torch.stack(all_layer_original_hiddens, 2)\n",
    "            all_original_hiddens = all_original_hiddens.detach()\n",
    "            if args.token_loss:\n",
    "                original_hiddens - original_hiddens.detach()\n",
    "                token_original = token_original.detach()\n",
    "        if args.debias_layer == 'all':\n",
    "            target_layer_hiddens = all_layer_hiddens\n",
    "            target_original_hiddens = all_layer_hiddens\n",
    "        else:\n",
    "            if args.debias_layer == 'first':\n",
    "                idx = 0\n",
    "            elif args.debias_layer == 'last':\n",
    "                idx = -1\n",
    "            target_layer_hiddens = all_layer_hiddens[:,:,idx]\n",
    "            target_layer_hiddens = target_layer_hiddens.unsqueeze(2)\n",
    "            if 'neutral' != key:\n",
    "                target_original_hiddens = all_original_hiddens[:,:,idx]\n",
    "                target_original_hiddens = target_original_hiddens.unsqueeze(2)\n",
    "            else:\n",
    "                attributes_hiddens = {key: value[:,idx,:].unsqueeze(1) for key, value in attributes_hiddens.items()}\n",
    "\n",
    "        if args.loss_target == 'sentence' or labels is None:\n",
    "            attributes_hiddens = {key: value.unsqueeze(1) for key, value in attributes_hiddens.items()}\n",
    "        #elif args.loss_target == 'token' and key == 'neutral':\n",
    "        elif args.loss_target == 'token':\n",
    "            if labels.size(1) > 1:\n",
    "                onehot = torch.eye(target_layer_hiddens.size(1))\n",
    "                zeros = torch.zeros(1, onehot.size(0))\n",
    "                onehot = torch.cat((zeros, onehot), 0)\n",
    "                onehot = onehot[labels]\n",
    "                onehot = torch.sum(onehot, 1)\n",
    "                onehot = onehot.view(target_layer_hiddens.size(0), -1, 1, 1)\n",
    "            else:\n",
    "                onehot = torch.eye(target_layer_hiddens.size(1))[labels].view(target_layer_hiddens.size(0), -1, 1, 1)\n",
    "            onehot = onehot.to(args.device)\n",
    "            target_layer_hiddens = torch.sum(target_layer_hiddens * onehot, 1).unsqueeze(1) / labels.size(1)\n",
    "            if 'neutral' != key:\n",
    "                target_original_hiddens = torch.sum(target_original_hiddens * onehot, 1).unsqueeze(1) / labels.size(1)\n",
    "            else:\n",
    "                attributes_hiddens = {key: value.expand(target_layer_hiddens.size(0), 1, value.size(1),value.size(2)) for key, value in attributes_hiddens.items()}\n",
    "\n",
    "        if 'neutral' == key:\n",
    "            loss = 0\n",
    "            for attribute_hiddens in attributes_hiddens.values():\n",
    "                tmp_loss = criterion_ip(target_layer_hiddens, attribute_hiddens)\n",
    "                if args.square_loss:\n",
    "                    tmp_loss = tmp_loss ** 2\n",
    "                tmp_loss *= alpha\n",
    "                loss += tmp_loss\n",
    "        else:\n",
    "            #loss = criterion_ms(target_layer_hiddens, target_original_hiddens)\n",
    "            loss = criterion_ms(all_layer_hiddens, all_original_hiddens, 3)\n",
    "            if args.token_loss:\n",
    "                loss += criterion_ms(token_predicts, token_original, 2)\n",
    "                #loss += criterion_ms(hiddens, original_hiddens, 2)\n",
    "            loss *= beta\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate(model, attributes_hiddens, dev_dataloaders, prefix=\"\"):\n",
    "        # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "        eval_output_dir = args.output_dir\n",
    "\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "        args.eval_batch_size=args.per_gpu_eval_batch_size*max(1,args.n_gpu)\n",
    "        # multi-gpu evaluate\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", dev_example_num)\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        for key in tqdm(dev_distribution):\n",
    "            with torch.no_grad():\n",
    "                loss = forward(attributes_hiddens, dev_dataloaders, key)\n",
    "\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                model.zero_grad()\n",
    "                original_model.zero_grad()\n",
    "\n",
    "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "            logger.info(\"  Loss = %s\", eval_loss)\n",
    "            writer.write(\"Loss = %s\\n\" % (eval_loss))\n",
    "\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    criterion_ms=mean_square\n",
    "    criterion_ip=inner_product\n",
    "    original_model.eval()\n",
    "\n",
    "    alpha, beta=args.weighted_loss\n",
    "    alpha=float(alpha)\n",
    "    beta=float(beta)\n",
    "\n",
    "    train_loss=0.0\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        random.shuffle(train_distribution)\n",
    "        epoch_iterator = tqdm(train_distribution, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            attributes_hiddens = attribute_vector_example()\n",
    "\n",
    "        for step, key in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            loss = forward(attributes_hiddens, train_dataloaders, key)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                original_model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    logger.info(\" global_step = %s, train loss = %s\", global_step, train_loss)\n",
    "                    train_loss = 0.0\n",
    "                    # Log metrics\n",
    "                    best_loss, best_step = save_best_model(best_loss, best_step, dev_dataloaders)\n",
    "                    dev_dataloaders, dev_example_num, dev_distribution = create_dataloader(args, dev_datasets, tokenizer, train=False)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "            train_dataloaders, train_example_num, train_distribution = create_dataloader(args, train_datasets, tokenizer, train=True)\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    dev_dataloaders, dev_example_num, dev_distribution = create_dataloader(args, dev_datasets, tokenizer, train=False)\n",
    "    best_loss, best_step = save_best_model(best_loss, best_step, dev_dataloaders)\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/11/2022 16:55:14 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "12/11/2022 16:55:24 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x000001EE96C01790>\n",
      "12/11/2022 16:55:26 - INFO - __main__ -   ***** Running training *****\n",
      "12/11/2022 16:55:26 - INFO - __main__ -     Num examples = 1800\n",
      "12/11/2022 16:55:26 - INFO - __main__ -     Num Epochs = 2\n",
      "12/11/2022 16:55:26 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
      "12/11/2022 16:55:26 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "12/11/2022 16:55:26 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/11/2022 16:55:26 - INFO - __main__ -     Total optimization steps = 116\n",
      "Iteration:  11%|█         | 6/56 [02:45<23:03, 27.67s/it]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3568/3345602388.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    135\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdistributed\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbarrier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 137\u001B[1;33m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msplited_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moriginal_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3568/4108609316.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(args, data, datasets, model, original_model, tokenizer)\u001B[0m\n\u001B[0;32m    496\u001B[0m                     \u001B[0mscaled_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    497\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 498\u001B[1;33m                 \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    499\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    500\u001B[0m             \u001B[0mtrain_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    394\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    395\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 396\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    397\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    398\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\31631\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[1;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m     \u001B[1;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 173\u001B[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[0;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    175\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if (\n",
    "    os.path.exists(args.output_dir)\n",
    "    and os.listdir(args.output_dir)\n",
    "    and args.do_train\n",
    "    and not args.overwrite_output_dir\n",
    "    and not args.should_continue\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Setup distant debugging if needed\n",
    "if args.server_ip and args.server_port:\n",
    "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "    import ptvsd\n",
    "\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "set_seed(args)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    args.local_rank,\n",
    "    device,\n",
    "    args.n_gpu,\n",
    "    bool(args.local_rank != -1),\n",
    "    args.fp16,\n",
    ")\n",
    "\n",
    " # Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
    "else:\n",
    "    # When we release a pip version exposing CONFIG_MAPPING,\n",
    "    # we can do `config = CONFIG_MAPPING[args.model_type]()`.\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new config instance from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "        \"and load it from here, using --config_name\"\n",
    "    )\n",
    "\n",
    "config.output_hidden_states = 'true'\n",
    "\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "        \"and load it from here, using --tokenizer_name\"\n",
    "    )\n",
    "\n",
    "if args.block_size <= 0:\n",
    "    args.block_size = tokenizer.model_max_length\n",
    "    # Our input block size will be the max possible for the model\n",
    "else:\n",
    "    args.block_size = min(args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "if args.model_name_or_path:\n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    original_model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelWithLMHead.from_config(config)\n",
    "    original_model = AutoModelWithLMHead.from_config(config)\n",
    "\n",
    "# GPT-2 and GPT do not have pad.\n",
    "if tokenizer._pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    original_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(args.device)\n",
    "original_model.to(args.device)\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "data = torch.load(args.data_file)\n",
    "\n",
    "attributes_examples = data['attributes_examples']\n",
    "attributes_labels = data['attributes_labels']\n",
    "neutral_examples = data['neutral_examples']\n",
    "\n",
    "if 'neutral_labels' in data:\n",
    "    neutral_labels = data['neutral_labels']\n",
    "    splited_data = split_data(attributes_examples, attributes_labels, neutral_examples, neutral_labels, args)\n",
    "else:\n",
    "    splited_data = split_data(attributes_examples, attributes_labels, neutral_examples, None, args)\n",
    "\n",
    "datasets = load_and_cache_examples(splited_data, args, tokenizer)\n",
    "\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "train(args, splited_data, datasets, model, original_model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}